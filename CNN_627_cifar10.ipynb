{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbb505f",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aff032f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T22:13:06.852976Z",
     "start_time": "2023-02-21T22:13:06.838976Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Library\n",
    "import sys\n",
    "import os\n",
    "import os.path as pth\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from quantization import *\n",
    "\n",
    "import nets\n",
    "import datasets\n",
    "import tools\n",
    "import layers as L\n",
    "\n",
    "#!pip install torchplot\n",
    "\n",
    "import torchplot as plt\n",
    "import math\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b153588",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed1606e4-d378-4cfe-92d4-9909afc39f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T22:12:11.202832Z",
     "start_time": "2023-02-21T22:12:11.187845Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "network = \"resnet18\"\n",
    "dataset = \"cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040d288",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd7928d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T22:12:11.836429Z",
     "start_time": "2023-02-21T22:12:11.822843Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(model, trainloader, testloader, args):\n",
    "    log = tools.StatLogger(args.log_path)\n",
    "\n",
    "    # ===================================\n",
    "    # initialize and run training session\n",
    "    # ===================================\n",
    "    # model.cuda()\n",
    "    lossfunc = nn.CrossEntropyLoss()#.cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.base_lr, momentum=0.9, weight_decay=args.weight_decay)\n",
    "\n",
    "    # retrive pruning masks\n",
    "    weight_masks = retrieve_masks(model)\n",
    "\n",
    "    # Apply quantization\n",
    "    index = 0\n",
    "    for n, m in model.named_modules():\n",
    "        if SFCL:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = quantize(m.weight.data, torch.max(torch.abs(m.weight.data)).item())\n",
    "                index += 1\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if m.kernel_size == (3, 3):\n",
    "                m.weight.data = quantize(m.weight.data, torch.max(torch.abs(m.weight.data)).item())\n",
    "                index += 1\n",
    "\n",
    "    # Retraining steps\n",
    "    for epoch in range(args.retraining_epochs):\n",
    "        epoch += 1\n",
    "        model.train()\n",
    "        error_top1 = []\n",
    "        error_top5 = []\n",
    "        running_loss = []\n",
    "\n",
    "        for idx, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            #inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = lossfunc(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # get masked weights\n",
    "            apply_mask(model, weight_masks)\n",
    "\n",
    "            error_top1.append(tools.topK_error(outputs, labels, K=1).item())\n",
    "            error_top5.append(tools.topK_error(outputs, labels, K=5).item())\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "        error_top1 = np.average(error_top1)\n",
    "        error_top5 = np.average(error_top5)\n",
    "        running_loss = np.average(running_loss)\n",
    "        # print statistics\n",
    "        print(\"RETRAIN epoch:%-4d error_top1: %.4f error_top5: %.4f loss:%.4f\" % (\n",
    "            epoch, error_top1, error_top5, running_loss))\n",
    "        log.report(epoch=epoch,\n",
    "                   split='RETRAIN',\n",
    "                   error_top5=float(error_top5),\n",
    "                   error_top1=float(error_top1),\n",
    "                   loss=float(running_loss))\n",
    "\n",
    "        # Quantize again\n",
    "        index = 0\n",
    "        for n, m in model.named_modules():\n",
    "            if SFCL:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    m.weight.data = quantize(m.weight.data, torch.max(torch.abs(m.weight.data)).item())\n",
    "                    index += 1\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m.kernel_size == (3, 3):\n",
    "                    m.weight.data = quantize(m.weight.data, torch.max(torch.abs(m.weight.data)).item())\n",
    "                    index += 1\n",
    "\n",
    "        validate(model, testloader, lossfunc, log, epoch)\n",
    "\n",
    "        print('-- saving model check point')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "        }, os.path.join(args.checkpoint_dir, 'checkpoint_{}.tar'.format(epoch)))\n",
    "\n",
    "    print('Finished Retraining')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe5df6",
   "metadata": {},
   "source": [
    "## Validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438b54f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T22:12:12.581287Z",
     "start_time": "2023-02-21T22:12:12.563711Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate(model, testloader, lossfunc, log, epoch):\n",
    "    print(\"-- running evaluation on validation split\")\n",
    "    model.eval()\n",
    "    error_top1 = []\n",
    "    error_top5 = []\n",
    "    vld_loss = []\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            #inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            error_top1.append(tools.topK_error(outputs, labels, K=1).item())\n",
    "            error_top5.append(tools.topK_error(outputs, labels, K=5).item())\n",
    "            vld_loss.append(lossfunc(outputs, labels).item())\n",
    "\n",
    "        error_top1 = np.average(error_top1)\n",
    "        error_top5 = np.average(error_top5)\n",
    "        vld_loss = np.average(vld_loss)\n",
    "        print(\n",
    "            \"VALID epoch:%-4d error_top1: %.4f error_top5: %.4f loss:%.4f\" % (epoch, error_top1, error_top5, vld_loss))\n",
    "        log.report(epoch=epoch,\n",
    "                   split='VALID',\n",
    "                   error_top5=float(error_top5),\n",
    "                   error_top1=float(error_top1),\n",
    "                   loss=float(vld_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2684ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T20:38:36.714552Z",
     "start_time": "2023-02-11T20:38:36.700552Z"
    }
   },
   "source": [
    "# Quantization my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88972c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T22:12:13.996122Z",
     "start_time": "2023-02-21T22:12:13.959485Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Need to modify original function\n",
    "def getQlevels(bits=6):\n",
    "    qlevels = [0]\n",
    "    itertable = []\n",
    "    for i in range(bits - 1):\n",
    "        qlevels.extend((2 ** i, -2 ** i))\n",
    "        itertable.extend((2 ** i, -2 ** i))\n",
    "    itertable.extend((2 ** (bits - 1), -2 ** (bits - 1)))\n",
    "    comb2 = list(itertools.combinations(itertable, 2))\n",
    "    for item in comb2:\n",
    "        val = item[0] + item[1]\n",
    "        if 2 ** (bits - 1) > val > -(2 ** (bits - 1)):\n",
    "            qlevels.append(val)\n",
    "    qlevels = sorted(list(dict.fromkeys(qlevels)))\n",
    "    return qlevels\n",
    "\n",
    "# Actual quantization function (Some how related to getQlevels function)\n",
    "def quantize(weights, bound):\n",
    "    weight = weights.detach().flatten()\n",
    "    weight[weight>bound] = bound\n",
    "    weight[weight<-bound] = -bound\n",
    "    alpha = bound/torch.max(_QLEVELS)\n",
    "    alpha = 2**(torch.round(torch.log2(alpha)))\n",
    "\n",
    "    # print(\"alpha: {:.4f}\".format(alpha))\n",
    "    #torch.cuda.empty_cache()\n",
    "    idx = torch.argmin(torch.abs((weight/alpha).unsqueeze(0) - _QLEVELS.unsqueeze(1)), dim=0)\n",
    "    weight_int = _QLEVELS[idx]\n",
    "    weight_quant = weight_int.float() * alpha\n",
    "    return weight_quant.reshape(weights.shape)\n",
    "\n",
    "# CSD quantization with quant_bits = 6\n",
    "# _QLEVELS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 20, 24, 28, 30, 31,\n",
    "#      -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -12, -14, -15, -16, -17, -18, -20, -24, -28, -30, -31]\n",
    "# _QLEVELS = getQlevels(6)\n",
    "# print(_QLEVELS)\n",
    "\n",
    "_QLEVELS = [i for i in range(-31, 31)]\n",
    "_QLEVELS = torch.tensor(_QLEVELS)#.cuda()\n",
    "\n",
    "# Quantization function from Pierre\n",
    "\n",
    "# Retrieve pruning masks\n",
    "def retrieve_masks(model):\n",
    "    num_pruned, num_weights = 0, 0\n",
    "    weight_masks = []\n",
    "    for m in model.modules():\n",
    "        if SFCL:\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, L.MultLayer) or isinstance(m, L.MultLayer3) or isinstance(m, nn.Linear):\n",
    "                num = torch.numel(m.weight.data)\n",
    "                weight_mask = (abs(m.weight.data) > 0).float()\n",
    "                weight_masks.append(weight_mask)\n",
    "\n",
    "                num_pruned += num - torch.sum(weight_mask)\n",
    "                num_weights += num\n",
    "        else:\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, L.MultLayer) or isinstance(m, L.MultLayer3):\n",
    "                num = torch.numel(m.weight.data)\n",
    "                weight_mask = (abs(m.weight.data) > 0).float()\n",
    "                weight_masks.append(weight_mask)\n",
    "\n",
    "                num_pruned += num - torch.sum(weight_mask)\n",
    "                num_weights += num\n",
    "\n",
    "    # print('-- compress rate: %.4f' % (num_pruned / num_weights))\n",
    "    return weight_masks\n",
    "\n",
    "# Apply Mask\n",
    "def apply_mask(model, weight_masks):\n",
    "    idx = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, L.MultLayer) or isinstance(m, L.MultLayer3):\n",
    "            m.weight.data *= weight_masks[idx]\n",
    "            idx += 1\n",
    "        if SFCL:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if m.out_features >= 16:\n",
    "                    m.weight.data *= weight_masks[idx]\n",
    "                    idx += 1\n",
    "\n",
    "# Get Q-levels (What is this?)\n",
    "def getQlevels(bits=6):\n",
    "    qlevels = [0]\n",
    "    itertable = []\n",
    "    for i in range(bits - 1):\n",
    "        qlevels.extend((2 ** i, -2 ** i))\n",
    "        itertable.extend((2 ** i, -2 ** i))\n",
    "    itertable.extend((2 ** (bits - 1), -2 ** (bits - 1)))\n",
    "    comb2 = list(itertools.combinations(itertable, 2))\n",
    "    for item in comb2:\n",
    "        val = item[0] + item[1]\n",
    "        if 2 ** (bits - 1) > val > -(2 ** (bits - 1)):\n",
    "            qlevels.append(val)\n",
    "    qlevels = sorted(list(dict.fromkeys(qlevels)))\n",
    "    return qlevels\n",
    "\n",
    "# Actual quantization function (Some how related to getQlevels function)\n",
    "def quantize(weights, bound):\n",
    "    weight = weights.detach().flatten()\n",
    "    weight[weight>bound] = bound\n",
    "    weight[weight<-bound] = -bound\n",
    "    alpha = bound/torch.max(_QLEVELS)\n",
    "    alpha = 2**(torch.round(torch.log2(alpha)))\n",
    "\n",
    "    # print(\"alpha: {:.4f}\".format(alpha))\n",
    "    #torch.cuda.empty_cache()\n",
    "    idx = torch.argmin(torch.abs((weight/alpha).unsqueeze(0) - _QLEVELS.unsqueeze(1)), dim=0)\n",
    "    weight_int = _QLEVELS[idx]\n",
    "    weight_quant = weight_int.float() * alpha\n",
    "    return weight_quant.reshape(weights.shape)\n",
    "\n",
    "# CSD quantization with quant_bits = 6\n",
    "# _QLEVELS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 20, 24, 28, 30, 31,\n",
    "#      -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -12, -14, -15, -16, -17, -18, -20, -24, -28, -30, -31]\n",
    "# _QLEVELS = getQlevels(6)\n",
    "# print(_QLEVELS)\n",
    "\n",
    "_QLEVELS = [i for i in range(-31, 31)]\n",
    "_QLEVELS = torch.tensor(_QLEVELS)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ebbb57c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T22:21:07.205472Z",
     "start_time": "2023-02-18T22:21:07.167751Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31.0000, 13.0000],\n",
      "        [ 1.2351,  1.0000]])\n",
      "tensor([31.0000, 13.0000,  1.2351,  1.0000])\n",
      "tensor([8.0000, 8.0000, 1.2351, 1.0000])\n",
      "tensor([8.0000, 8.0000, 1.2351, 1.0000])\n",
      "tensor([[8.0000, 8.0000],\n",
      "        [1.2351, 1.0000]])\n",
      "tensor([[7.5000, 7.5000],\n",
      "        [1.2500, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Practice of above function\n",
    "_QLEVELS = [i for i in range(-31, 31)]\n",
    "_QLEVELS = torch.tensor(_QLEVELS)#.cuda()\n",
    "a = torch.tensor([[31, 13], [1.2351, 1]])\n",
    "weights = a\n",
    "bound = 8\n",
    "print(weights)\n",
    "weight = weights.detach().flatten()\n",
    "print(weight)\n",
    "weight[weight>bound] = bound\n",
    "print(weight)\n",
    "weight[weight<-bound] = -bound\n",
    "print(weight)\n",
    "\n",
    "weight = weights.detach().flatten()\n",
    "weight[weight>bound] = bound\n",
    "weight[weight<-bound] = -bound\n",
    "alpha = bound/torch.max(_QLEVELS)\n",
    "alpha = 2**(torch.round(torch.log2(alpha)))\n",
    "\n",
    "# print(\"alpha: {:.4f}\".format(alpha))\n",
    "#torch.cuda.empty_cache()\n",
    "idx = torch.argmin(torch.abs((weight/alpha).unsqueeze(0) - _QLEVELS.unsqueeze(1)), dim=0)\n",
    "weight_int = _QLEVELS[idx]\n",
    "weight_quant = weight_int.float() * alpha\n",
    "\n",
    "\n",
    "print(a)\n",
    "c = quantize(a, 8)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddd78033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T22:27:20.237267Z",
     "start_time": "2023-02-18T22:27:20.220503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quant with 1 integer bit\n",
    "def quant_signed_1(original, bit=6):\n",
    "    bit = bit - 2\n",
    "    original = original.clamp(max=1.9375,min=-1.9375)\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return ((original * (2**bit)).int()) / (2**bit)\n",
    "\n",
    "\n",
    "# Quant with no integer bit\n",
    "def quant_signed_0(original, bit=6):\n",
    "    bit = bit - 1\n",
    "    original = original.clamp(max=0.96875,min=-0.96875)\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return ((original * (2**bit)).int()) / (2**bit)\n",
    "\n",
    "def quant_signed_15(original, bit=5):\n",
    "    bit = bit - 2\n",
    "    original = original.clamp(max=1.875,min=-1.875)\n",
    "    #print(\"Activation result quantization\")\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return ((original * (2**bit)).int()) / (2**bit)\n",
    "\n",
    "\n",
    "def quant_signed_05(original, bit=5):\n",
    "    bit = bit - 1\n",
    "    original = original.clamp(max=0.9375,min=-0.9375)\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return ((original * (2**bit)).int()) / (2**bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94bea135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T22:28:44.512107Z",
     "start_time": "2023-02-18T22:28:44.501141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.0234],\n",
      "        [0.2351, 0.5000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.50000, 0.00000],\n",
       "        [0.21875, 0.50000]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0.5, 0.0234], [0.2351, 0.5]])\n",
    "print(a)\n",
    "quant_signed_0(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cecc4119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T22:29:26.630109Z",
     "start_time": "2023-02-18T22:29:26.623692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.50000, 0.02340],\n",
      "        [0.23510, 0.50000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[16.00000,  0.74880],\n",
       "        [ 7.52320, 16.00000]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.clamp(max=0.96875,min=-0.96875)\n",
    "print(b)\n",
    "b * (32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fd5a5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T22:33:05.809154Z",
     "start_time": "2023-02-18T22:33:05.797785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.01000, 0.02340],\n",
      "        [0.23510, 0.01200]])\n",
      "tensor([[0.00000, 0.03125],\n",
      "        [0.25000, 0.00000]])\n",
      "tensor([[0., 1.],\n",
      "        [8., 0.]])\n",
      "tensor([[0.00000, 0.00000],\n",
      "        [0.21875, 0.00000]])\n",
      "tensor([[0., 0.],\n",
      "        [7., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[0.01, 0.0234], [0.2351, 0.012]])\n",
    "print(a)\n",
    "c = quantize(a, 0.96875)\n",
    "print(c)\n",
    "print(c*32)\n",
    "d = quant_signed_0(a)\n",
    "print(d)\n",
    "print(d*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "887f0a06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T22:26:40.345473Z",
     "start_time": "2023-02-18T22:26:40.332224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21875"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd06c2d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T22:26:25.025558Z",
     "start_time": "2023-02-18T22:26:25.018108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_signed_0(torch.tensor([[0.0234]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2bf64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29937c18",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07c7dd39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T07:18:02.048578Z",
     "start_time": "2023-02-15T07:18:02.031529Z"
    },
    "code_folding": [
     1,
     7,
     19,
     42,
     62
    ]
   },
   "outputs": [],
   "source": [
    "# All function for evaulation\n",
    "def dump_act(module, input, output):\n",
    "    if len(output) > 0:\n",
    "        input_act_list.append(input[0].detach().cpu().numpy())\n",
    "        output_act_list.append(output[0].detach().cpu().numpy())\n",
    "\n",
    "# Calculate weight density\n",
    "def cal_density(model):\n",
    "    num_pruned, num_weights = 0, 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d) or isinstance(m, L.MultLayer) or isinstance(m, L.MultLayer3):\n",
    "            num = torch.numel(m.weight.data)\n",
    "            weight_mask = (abs(m.weight.data) > 0).float()\n",
    "\n",
    "            num_pruned += num - torch.sum(weight_mask)\n",
    "            num_weights += num\n",
    "\n",
    "    return 1 - num_pruned / num_weights\n",
    "\n",
    "def eval(model, testloader):\n",
    "    print(\"Running evaluation on validation split\")\n",
    "    model.eval()\n",
    "\n",
    "    lossfunc = nn.CrossEntropyLoss().cuda()\n",
    "    error_top1 = []\n",
    "    error_top5 = []\n",
    "    vld_loss = []\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            error_top1.append(tools.topK_error(outputs, labels, K=1).item())\n",
    "            error_top5.append(tools.topK_error(outputs, labels, K=5).item())\n",
    "            vld_loss.append(lossfunc(outputs, labels).item())\n",
    "\n",
    "        error_top1 = np.average(error_top1)\n",
    "        error_top5 = np.average(error_top5)\n",
    "        vld_loss = np.average(vld_loss)\n",
    "        print(\"-- Validation result -- acc_top1: %.4f acc_top5: %.4f loss:%.4f\" % (1-error_top1, 1-error_top5, vld_loss))\n",
    "\n",
    "def dump_batch(model, testloader, batch_size, arch):\n",
    "    print(\"Dumping batch for simulation\")\n",
    "    for n, m in model.named_modules():\n",
    "        # if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d) or isinstance(m, L.MultLayer) or isinstance(m, L.MultLayer3):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if m.kernel_size == (3, 3):\n",
    "                m.register_forward_hook(dump_act)\n",
    "                weight_list.append(m.weight.data.detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs = data[0][0:batch_size, :, :, :]\n",
    "            inputs = inputs.cuda()\n",
    "            model(inputs)\n",
    "            break\n",
    "\n",
    "    for i in range(0, len(weight_list)):\n",
    "        np.save(\"../py_sim_dump/{}/wgt-layer_{}.npy\".format(arch, i), weight_list[i])\n",
    "        np.save(\"../py_sim_dump/{}/act-layer_{}-{}.npy\".format(arch, i, batch_size), input_act_list[i])\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.arch == 'resnet20':\n",
    "        model = nets.RESNET20MULT3()\n",
    "        pretrained_checkpoint = '../checkpoints/original/resnet20_ckpt.tar'\n",
    "    if args.arch == 'vggnagamult':\n",
    "        model = nets.VGGnagaMULT()\n",
    "        pretrained_checkpoint = '../checkpoints/vggnagamult_quant_8bit_ckpt.tar'\n",
    "    if args.arch == 'cnnc':\n",
    "        model = nets.CNNCMULT3()\n",
    "        pretrained_checkpoint = '../checkpoints/original/cnnc_ckpt.tar'\n",
    "    if args.arch == 'cnnc-conv':\n",
    "        model = nets.CNNC()\n",
    "        pretrained_checkpoint = '../checkpoints/cnnc-conv_quant_8bit_ckpt.tar'\n",
    "    if args.arch == 'vggnagacnn':\n",
    "        model = nets.VGGnagaCNN()\n",
    "        pretrained_checkpoint = '../checkpoints/vggnagacnn_unstructured_quant_8bit_sfcl_ckpt.tar'\n",
    "    if args.arch == 'resnet18cnn':\n",
    "        model = nets.RESNET18CNN()\n",
    "        pretrained_checkpoint = '../quant_cnn_checkpoints/resnet18_cifar10/checkpoint_30.tar'\n",
    "    if args.arch == 'resnet20cnn':\n",
    "        model = nets.RESNET20CNN()\n",
    "        pretrained_checkpoint = '../checkpoints/resnet20cnn_quant_8bit_ckpt.tar'\n",
    "\n",
    "    model.cuda()\n",
    "\n",
    "    # load pretrained checkpoint\n",
    "\n",
    "    \n",
    "    print(\"Loading checkpoint '{}'\".format(pretrained_checkpoint))\n",
    "    pretrained_ckpt = torch.load(pretrained_checkpoint)\n",
    "    print(pretrained_ckpt)\n",
    "    model.load_state_dict(pretrained_ckpt['state_dict'])\n",
    "    print(\"Loaded checkpoint '{}'\".format(pretrained_checkpoint))\n",
    "\n",
    "    density = cal_density(model)\n",
    "    print(\"-- Weight density after learning sparsity and CSD quantization: %.4f\" % density)\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        _, _, testloader = datasets.get_cifar10()\n",
    "    if args.dataset == 'cifar100':\n",
    "        _, _, testloader = datasets.get_cifar100()\n",
    "    if args.dataset == 'imagenet':\n",
    "        _, _, testloader = datasets.get_imagenet(args.dataset_dir)\n",
    "\n",
    "\n",
    "    # dump_batch(model, testloader, 1, args.arch)\n",
    "    eval(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d30702",
   "metadata": {},
   "source": [
    "## Weights values & plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7687e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T07:18:02.891044Z",
     "start_time": "2023-02-15T07:18:02.708867Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "Loading checkpoint './checkpoints_train/CNN_627_cifar10/Sun_Feb_12_15_59_00_2023/checkpoint_200.tar'\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './checkpoints_train/CNN_627_cifar10/Sun_Feb_12_15_59_00_2023/checkpoint_200.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pretrained_checkpoint))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m pretrained_ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#print(pretrained_ckpt)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(pretrained_ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checkpoints_train/CNN_627_cifar10/Sun_Feb_12_15_59_00_2023/checkpoint_200.tar'"
     ]
    }
   ],
   "source": [
    "# Weights values and plot\n",
    "\n",
    "import torchplot as plt\n",
    "\n",
    "batch_size = 30\n",
    "dataset = 'cifar10'\n",
    "arch = 'CNN_627'\n",
    "pretrained_checkpoint = './checkpoints_train/CNN_627_cifar10/Sun_Feb_12_15_59_00_2023/checkpoint_200.tar'\n",
    "model = nets.CNN_627()\n",
    "\n",
    "\n",
    "#model.cuda()\n",
    "\n",
    "# load pretrained checkpoint\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"Loading checkpoint '{}'\".format(pretrained_checkpoint))\n",
    "print(\"-----------------------------------------------------\")\n",
    "pretrained_ckpt = torch.load(pretrained_checkpoint)\n",
    "#print(pretrained_ckpt)\n",
    "model.load_state_dict(pretrained_ckpt['state_dict'])\n",
    "\n",
    "weights = pretrained_ckpt\n",
    "#model.load_state_dict(pretrained_ckpt['state_dict'])\n",
    "#print(pretrained_ckpt.keys())\n",
    "#print(\"-----------------------------------------------------\")\n",
    "#print(pretrained_ckpt['state_dict'].keys())\n",
    "for k in pretrained_ckpt['state_dict'].keys():\n",
    "    print(k)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(pretrained_ckpt['state_dict']['conv1_1.weight'].shape)\n",
    "#print(pretrained_ckpt['state_dict']['conv1_1.weight'])\n",
    "print(\"-----------------------------------------------------\")\n",
    "weight1 = pretrained_ckpt['state_dict']['conv1_1.weight']\n",
    "weight2 = pretrained_ckpt['state_dict']['conv2_1.weight']\n",
    "weight3 = pretrained_ckpt['state_dict']['conv3_1.weight']\n",
    "weight4 = pretrained_ckpt['state_dict']['conv4_1.weight']\n",
    "weight5 = pretrained_ckpt['state_dict']['fc5.weight']\n",
    "weight6 = pretrained_ckpt['state_dict']['fc6.weight']\n",
    "\n",
    "weight = [weight1, weight2, weight3, weight4, weight5, weight6]\n",
    "print('     min max of conv layers weights')\n",
    "print(\"-----------------------------------------------------\")\n",
    "print('         max               min')\n",
    "for w in weight:\n",
    "    max_num = -999\n",
    "    min_num = 999\n",
    "    print(torch.max(w).item(), torch.min(w).item())\n",
    "t1 = torch.flatten(weight1)\n",
    "t2 = torch.flatten(weight2)\n",
    "t3 = torch.flatten(weight3)\n",
    "t4 = torch.flatten(weight4)\n",
    "t5 = torch.flatten(weight5)\n",
    "#plt.plot(t1)\n",
    "#plt.plot(t2)\n",
    "#plt.plot(t3)\n",
    "plt.plot(t4)\n",
    "#plt.plot(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f4b28e",
   "metadata": {},
   "source": [
    "## Quantization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f73599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T21:58:44.119190Z",
     "start_time": "2023-02-15T21:58:44.095182Z"
    }
   },
   "outputs": [],
   "source": [
    "def getQlevels(bits=6):\n",
    "    qlevels = [0]\n",
    "    itertable = []\n",
    "    for i in range(bits - 1):\n",
    "        qlevels.extend((2 ** i, -2 ** i))\n",
    "        itertable.extend((2 ** i, -2 ** i))\n",
    "    itertable.extend((2 ** (bits - 1), -2 ** (bits - 1)))\n",
    "    comb2 = list(itertools.combinations(itertable, 2))\n",
    "    for item in comb2:\n",
    "        val = item[0] + item[1]\n",
    "        if 2 ** (bits - 1) > val > -(2 ** (bits - 1)):\n",
    "            qlevels.append(val)\n",
    "    qlevels = sorted(list(dict.fromkeys(qlevels)))\n",
    "    return qlevels\n",
    "\n",
    "\n",
    "def quantize(weights, bound):\n",
    "    weight = weights.detach().flatten()\n",
    "    weight[weight>bound] = bound\n",
    "    weight[weight<-bound] = -bound\n",
    "    alpha = bound/torch.max(_QLEVELS)\n",
    "    alpha = 2**(torch.round(torch.log2(alpha)))\n",
    "\n",
    "    # print(\"alpha: {:.4f}\".format(alpha))\n",
    "    #torch.cuda.empty_cache()\n",
    "    idx = torch.argmin(torch.abs((weight/alpha).unsqueeze(0) - _QLEVELS.unsqueeze(1)), dim=0)\n",
    "    weight_int = _QLEVELS[idx]\n",
    "    weight_quant = weight_int.float() * alpha\n",
    "    return weight_quant.reshape(weights.shape)\n",
    "    \n",
    "_QLEVELS = [i for i in range(-31, 31)]\n",
    "_QLEVELS = torch.tensor(_QLEVELS)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3c22a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T21:59:30.751383Z",
     "start_time": "2023-02-15T21:59:30.726815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.8750, 1.8750, 0.1250, 0.3125])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = getQlevels()\n",
    "a = torch.tensor([1,2,3, 0.15, 0.3])\n",
    "b = [1,2,3]\n",
    "quantize(a,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71daaa9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5ab72aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T07:18:03.484683Z",
     "start_time": "2023-02-15T07:18:03.470616Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Quantization function\n",
    "# With 1bit for integer \n",
    "def quant_signed_1(original, bit=6):\n",
    "    bit = bit-2\n",
    "    a = original\n",
    "    depth = len(a.shape)\n",
    "    max_num = -999\n",
    "    min_num = 999\n",
    "    max_num = torch.max(a).item()\n",
    "    min_num = torch.min(a).item()\n",
    "    max_total = max(max_num, -min_num)\n",
    "    #print(max_total)\n",
    "\n",
    "    a = torch.flatten(a)\n",
    "    length = len(a)\n",
    "    output = []\n",
    "\n",
    "    for i in range(len(a)):\n",
    "        if a[i].item() >= 1:\n",
    "            val = 1\n",
    "        elif a[i].item() <= -1:\n",
    "            val = -1\n",
    "        else:\n",
    "            val = math.trunc(a[i].item()*(2**bit))/(2**bit)\n",
    "        output.append(val)\n",
    "\n",
    "    output = torch.tensor(output, dtype=torch.float64)\n",
    "    output = torch.reshape(output, original.shape)\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return output\n",
    "    #print(output)\n",
    "    #print(output * 2**bit)\n",
    "    \n",
    "# Without integer bit\n",
    "def quant_signed_0(original, bit=6):\n",
    "    bit = bit-1\n",
    "    a = original\n",
    "\n",
    "    a = torch.flatten(a)\n",
    "    length = len(a)\n",
    "    output = []\n",
    "\n",
    "    for i in range(len(a)):\n",
    "        if a[i].item() >= 0.96875:\n",
    "            val = 0.96875\n",
    "        elif a[i].item() <= -0.96875:\n",
    "            val = -0.96875\n",
    "        else:\n",
    "            val = math.trunc(a[i].item()*(2**bit))/(2**bit)\n",
    "        output.append(val)\n",
    "\n",
    "    output = torch.tensor(output, dtype=torch.float64)\n",
    "    output = torch.reshape(output, original.shape)\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    output = output.float()\n",
    "    return output\n",
    "    #print(output)\n",
    "    #print(output * 2**bit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfa9ccac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T22:26:06.597504Z",
     "start_time": "2023-02-18T22:26:06.576488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quant with 1 integer bit\n",
    "def quant_signed_1(original, bit=6):\n",
    "    bit = bit - 2\n",
    "    original = original.clamp(max=1.9375,min=-1.9375)\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return ((original * (2**bit)).int()) / (2**bit)\n",
    "\n",
    "\n",
    "# Quant with no integer bit\n",
    "def quant_signed_0(original, bit=6):\n",
    "    bit = bit - 1\n",
    "    original = original.clamp(max=0.96875,min=-0.96875)\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return ((original * (2**bit)).int()) / (2**bit)\n",
    "\n",
    "def quant_signed_15(original, bit=5):\n",
    "    bit = bit - 2\n",
    "    original = original.clamp(max=1.875,min=-1.875)\n",
    "    #print(\"Activation result quantization\")\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return ((original * (2**bit)).int()) / (2**bit)\n",
    "\n",
    "\n",
    "def quant_signed_05(original, bit=5):\n",
    "    bit = bit - 1\n",
    "    original = original.clamp(max=0.9375,min=-0.9375)\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return ((original * (2**bit)).int()) / (2**bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c81da0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T21:12:05.172620Z",
     "start_time": "2023-02-18T21:12:05.164018Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "75834624",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T01:12:19.268101Z",
     "start_time": "2023-02-12T01:12:19.245678Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     min max of fc layers weights\n",
      "-----------------------------------------------------\n",
      "         max               min\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Just an example of some function\n",
    "weight = [weight5, weight6]\n",
    "print('     min max of fc layers weights')\n",
    "print(\"-----------------------------------------------------\")\n",
    "print('         max               min')\n",
    "for w in weight:\n",
    "    max_num = -999\n",
    "    min_num = 999\n",
    "    #print(len(w))\n",
    "    for i in w:\n",
    "        #print(i)\n",
    "        num = i[0].item()\n",
    "        max_num = max(max_num,num)\n",
    "        min_num = min(min_num,num)\n",
    "    #print(max_num, min_num)\n",
    "print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60d8166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Weight density after learning sparsity and CSD quantization: 0.6584\n"
     ]
    }
   ],
   "source": [
    "density = cal_density(model)\n",
    "print(\"-- Weight density after learning sparsity and CSD quantization: %.4f\" % density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b9953",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'cifar10':\n",
    "    _, _, testloader = datasets.get_cifar10()\n",
    "if args.dataset == 'cifar100':\n",
    "    _, _, testloader = datasets.get_cifar100()\n",
    "if args.dataset == 'imagenet':\n",
    "    _, _, testloader = datasets.get_imagenet(args.dataset_dir)\n",
    "\n",
    "\n",
    "# dump_batch(model, testloader, 1, args.arch)\n",
    "eval(model, testloader)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf29759",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# load pretrained checkpoint\n",
    "if args.pretrained_weights is not None:\n",
    "    print(\"Loading checkpoint '{}'\".format(args.pretrained_weights))\n",
    "    pretrained_ckpt = torch.load(args.pretrained_weights)\n",
    "    model.load_state_dict(pretrained_ckpt['state_dict'])\n",
    "    print(\"Loaded checkpoint '{}'\".format(args.pretrained_weights))\n",
    "\n",
    "# setup checkpoint directory\n",
    "if not pth.exists(args.checkpoint_dir):\n",
    "    os.makedirs(args.checkpoint_dir)\n",
    "\n",
    "train(model, trainloader, testloader, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e08099",
   "metadata": {},
   "source": [
    "# Post-Quantization training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1236e485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T23:18:53.975618Z",
     "start_time": "2023-02-18T23:18:53.919303Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b908fbb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T21:08:57.342402Z",
     "start_time": "2023-02-18T21:08:57.300167Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Post-Quantization old function\n",
    "# CSD quantization with quant_bits = 6\n",
    "# _QLEVELS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 16, 17, 18, 20, 24, 28, 30, 31,\n",
    "#      -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -12, -14, -15, -16, -17, -18, -20, -24, -28, -30, -31]\n",
    "# _QLEVELS = getQlevels(6)\n",
    "# print(_QLEVELS)\n",
    "\n",
    "_QLEVELS = [i for i in range(-31, 31)]\n",
    "_QLEVELS = torch.tensor(_QLEVELS)#.cuda()\n",
    "\n",
    "# Retrieve pruning masks\n",
    "def retrieve_masks(model):\n",
    "    num_pruned, num_weights = 0, 0\n",
    "    weight_masks = []\n",
    "    for m in model.modules():\n",
    "        if SFCL:\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, L.MultLayer) or isinstance(m, L.MultLayer3) or isinstance(m, nn.Linear):\n",
    "                num = torch.numel(m.weight.data)\n",
    "                weight_mask = (abs(m.weight.data) > 0).float()\n",
    "                weight_masks.append(weight_mask)\n",
    "\n",
    "                num_pruned += num - torch.sum(weight_mask)\n",
    "                num_weights += num\n",
    "        else:\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, L.MultLayer) or isinstance(m, L.MultLayer3):\n",
    "                num = torch.numel(m.weight.data)\n",
    "                weight_mask = (abs(m.weight.data) > 0).float()\n",
    "                weight_masks.append(weight_mask)\n",
    "\n",
    "                num_pruned += num - torch.sum(weight_mask)\n",
    "                num_weights += num\n",
    "\n",
    "    # print('-- compress rate: %.4f' % (num_pruned / num_weights))\n",
    "    return weight_masks\n",
    "\n",
    "\n",
    "def apply_mask(model, weight_masks):\n",
    "    idx = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, L.MultLayer) or isinstance(m, L.MultLayer3):\n",
    "            m.weight.data *= weight_masks[idx]\n",
    "            idx += 1\n",
    "        if SFCL:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if m.out_features >= 16:\n",
    "                    m.weight.data *= weight_masks[idx]\n",
    "                    idx += 1\n",
    "\n",
    "def getQlevels(bits=6):\n",
    "    qlevels = [0]\n",
    "    itertable = []\n",
    "    for i in range(bits - 1):\n",
    "        qlevels.extend((2 ** i, -2 ** i))\n",
    "        itertable.extend((2 ** i, -2 ** i))\n",
    "    itertable.extend((2 ** (bits - 1), -2 ** (bits - 1)))\n",
    "    comb2 = list(itertools.combinations(itertable, 2))\n",
    "    for item in comb2:\n",
    "        val = item[0] + item[1]\n",
    "        if 2 ** (bits - 1) > val > -(2 ** (bits - 1)):\n",
    "            qlevels.append(val)\n",
    "    qlevels = sorted(list(dict.fromkeys(qlevels)))\n",
    "    return qlevels\n",
    "\n",
    "\n",
    "def quantize(weights, bound):\n",
    "    weight = weights.detach().flatten()\n",
    "    weight[weight>bound] = bound\n",
    "    weight[weight<-bound] = -bound\n",
    "    alpha = bound/torch.max(_QLEVELS)\n",
    "    alpha = 2**(torch.round(torch.log2(alpha)))\n",
    "\n",
    "    # print(\"alpha: {:.4f}\".format(alpha))\n",
    "    #torch.cuda.empty_cache()\n",
    "    idx = torch.argmin(torch.abs((weight/alpha).unsqueeze(0) - _QLEVELS.unsqueeze(1)), dim=0)\n",
    "    weight_int = _QLEVELS[idx]\n",
    "    weight_quant = weight_int.float() * alpha\n",
    "    return weight_quant.reshape(weights.shape)\n",
    "\n",
    "\n",
    "\n",
    "def train(model, trainloader, testloader, args):\n",
    "    log = tools.StatLogger(args.log_path)\n",
    "\n",
    "    # ===================================\n",
    "    # initialize and run training session\n",
    "    # ===================================\n",
    "    # model.cuda()\n",
    "    lossfunc = nn.CrossEntropyLoss()#.cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.base_lr, momentum=0.9, weight_decay=args.weight_decay)\n",
    "\n",
    "    # retrive pruning masks\n",
    "    weight_masks = retrieve_masks(model)\n",
    "\n",
    "    # Apply quantization\n",
    "    index = 0\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if m.kernel_size == (3, 3):\n",
    "                m.weight.data = quant_signed_1(m.weight.data, 6)\n",
    "                index += 1\n",
    "\n",
    "    # Retraining steps\n",
    "    for epoch in range(args.retraining_epochs):\n",
    "        epoch += 1\n",
    "        model.train()\n",
    "        error_top1 = []\n",
    "        error_top5 = []\n",
    "        running_loss = []\n",
    "\n",
    "        for idx, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            #inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = lossfunc(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # get masked weights\n",
    "            apply_mask(model, weight_masks)\n",
    "\n",
    "            error_top1.append(tools.topK_error(outputs, labels, K=1).item())\n",
    "            error_top5.append(tools.topK_error(outputs, labels, K=5).item())\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "        error_top1 = np.average(error_top1)\n",
    "        error_top5 = np.average(error_top5)\n",
    "        running_loss = np.average(running_loss)\n",
    "        # print statistics\n",
    "        print(\"RETRAIN epoch:%-4d error_top1: %.4f error_top5: %.4f loss:%.4f\" % (\n",
    "            epoch, error_top1, error_top5, running_loss))\n",
    "        log.report(epoch=epoch,\n",
    "                   split='RETRAIN',\n",
    "                   error_top5=float(error_top5),\n",
    "                   error_top1=float(error_top1),\n",
    "                   loss=float(running_loss))\n",
    "\n",
    "        # Quantize again\n",
    "        index = 0\n",
    "        for n, m in model.named_modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m.kernel_size == (3, 3):\n",
    "                    m.weight.data = quant_signed_1(m.weight.data, 6)\n",
    "                    index += 1\n",
    "\n",
    "        validate(model, testloader, lossfunc, log, epoch)\n",
    "\n",
    "        print('-- saving model check point')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "        }, os.path.join(args.checkpoint_dir, 'checkpoint_{}.tar'.format(epoch)))\n",
    "\n",
    "    print('Finished Retraining')\n",
    "\n",
    "\n",
    "def validate(model, testloader, lossfunc, log, epoch):\n",
    "    print(\"-- running evaluation on validation split\")\n",
    "    model.eval()\n",
    "    error_top1 = []\n",
    "    error_top5 = []\n",
    "    vld_loss = []\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            #inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            error_top1.append(tools.topK_error(outputs, labels, K=1).item())\n",
    "            error_top5.append(tools.topK_error(outputs, labels, K=5).item())\n",
    "            vld_loss.append(lossfunc(outputs, labels).item())\n",
    "\n",
    "        error_top1 = np.average(error_top1)\n",
    "        error_top5 = np.average(error_top5)\n",
    "        vld_loss = np.average(vld_loss)\n",
    "        print(\n",
    "            \"VALID epoch:%-4d error_top1: %.4f error_top5: %.4f loss:%.4f\" % (epoch, error_top1, error_top5, vld_loss))\n",
    "        log.report(epoch=epoch,\n",
    "                   split='VALID',\n",
    "                   error_top5=float(error_top5),\n",
    "                   error_top1=float(error_top1),\n",
    "                   loss=float(vld_loss))\n",
    "        \n",
    "        \n",
    "# Quantization function\n",
    "# With 1bit for integer \n",
    "def quant_signed_1(original, bit):\n",
    "    bit = bit-2\n",
    "    a = original\n",
    "    depth = len(a.shape)\n",
    "    max_num = -999\n",
    "    min_num = 999\n",
    "    max_num = torch.max(a).item()\n",
    "    min_num = torch.min(a).item()\n",
    "    max_total = max(max_num, -min_num)\n",
    "    #print(max_total)\n",
    "\n",
    "    a = torch.flatten(a)\n",
    "    length = len(a)\n",
    "    output = []\n",
    "\n",
    "    for i in range(len(a)):\n",
    "        if a[i].item() >= 1:\n",
    "            val = 1\n",
    "        else:\n",
    "            val = math.trunc(a[i].item()*(2**bit))/(2**bit)\n",
    "        output.append(val)\n",
    "\n",
    "    output = torch.tensor(output, dtype=torch.float64)\n",
    "    output = torch.reshape(output, original.shape)\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return output\n",
    "    #print(output)\n",
    "    #print(output * 2**bit)\n",
    "    \n",
    "# Without integer bit\n",
    "def quant_signed_0(original, bit=6):\n",
    "    bit = bit-1\n",
    "    a = original\n",
    "    \n",
    "    a = torch.flatten(a)\n",
    "    length = len(a)\n",
    "    output = []\n",
    "\n",
    "    for i in range(len(a)):\n",
    "        if a[i].item() >= 0.96875:\n",
    "            val = 0.96875\n",
    "        else:\n",
    "            val = math.trunc(a[i].item()*(2**bit))/(2**bit)\n",
    "        output.append(val)\n",
    "\n",
    "    output = torch.tensor(output, dtype=torch.float64)\n",
    "    output = torch.reshape(output, original.shape)\n",
    "    torch.set_printoptions(precision=bit)\n",
    "    return output\n",
    "    #print(output)\n",
    "    #print(output * 2**bit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12dfd7",
   "metadata": {},
   "source": [
    "# Getting Data and Save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcf202",
   "metadata": {},
   "source": [
    "## Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1db240c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T23:23:13.366267Z",
     "start_time": "2023-02-18T23:23:11.455972Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cifar10 data ... \n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Cifar10\n",
    "batch_size=256\n",
    "distributed=None\n",
    "trainsampler = None\n",
    "workers=2\n",
    "print(\"Loading cifar10 data ... \")\n",
    "transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomCrop(32, 4),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/tmpssd/pabillam/data', train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=(trainsampler is None), \n",
    "              num_workers=workers, pin_memory=True, sampler=trainsampler)\n",
    "\n",
    "val_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/tmpssd/pabillam/data', train=False, download=True, transform=val_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=False, num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2110e0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-18T23:22:30.004875Z",
     "start_time": "2023-02-18T23:22:29.974875Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m data_train \u001b[38;5;241m=\u001b[39m trainset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m label_train \u001b[38;5;241m=\u001b[39m trainset[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m data_train \u001b[38;5;241m=\u001b[39m (\u001b[43mdata_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m128\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m128\u001b[39m\n\u001b[0;32m      6\u001b[0m data_train \u001b[38;5;241m=\u001b[39m quant_signed_0(data_train)\n\u001b[0;32m      7\u001b[0m data_train \u001b[38;5;241m=\u001b[39m m(data_train)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'int'"
     ]
    }
   ],
   "source": [
    "# Save train data\n",
    "data_train = trainset[0]\n",
    "label_train = trainset[1]\n",
    "\n",
    "data_train = (data_train.int() - 128)/128\n",
    "data_train = quant_signed_0(data_train)\n",
    "\n",
    "\n",
    "with open(\"./data_quantized/quant_data.pkl\",\"wb\") as f:\n",
    "    pickle.dump(data_train, f)\n",
    "with open(\"./data_quantized/quant_label.pkl\",\"wb\") as g:\n",
    "    pickle.dump(label_train, g)\n",
    "    \n",
    "    \n",
    "# Save test data\n",
    "label = testset[0]\n",
    "data = testset[1]\n",
    "\n",
    "data = (data.int() - 128)/128\n",
    "data = quant_signed_0(data)\n",
    "data = m(data)\n",
    "\n",
    "with open(\"./data_quantized/quant_test_data.pkl\",\"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "with open(\"./data_quantized/quant_test_label.pkl\",\"wb\") as g:\n",
    "    pickle.dump(label, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5188b",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114f0657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T02:12:41.642574Z",
     "start_time": "2023-02-22T02:12:41.557385Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data ... \n"
     ]
    }
   ],
   "source": [
    "# MNIST\n",
    "batch_size=256\n",
    "distributed=None\n",
    "trainsampler = None\n",
    "workers=2\n",
    "\n",
    "print(\"Loading MNIST data ... \")\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./MNIST', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=(trainsampler is None), \n",
    "              num_workers=workers, pin_memory=True, sampler=trainsampler)\n",
    "\n",
    "val_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,))])\n",
    "testset = torchvision.datasets.MNIST(root='./MNIST', train=False, download=True, transform=val_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=False, num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39fa2cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T02:12:49.356146Z",
     "start_time": "2023-02-22T02:12:48.255837Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Saving Data\n",
    "\n",
    "# Padding\n",
    "m = nn.ConstantPad2d(2, 0)\n",
    "\n",
    "\n",
    "# Save train data\n",
    "data_train = trainset.train_data\n",
    "label_train = trainset.train_labels\n",
    "\n",
    "data_train = (data_train.int()-128)/128\n",
    "data_train = quant_signed_15(data_train)\n",
    "data_train = m(data_train)\n",
    "\n",
    "with open(\"./data_quantized/quant_data_mnist.pkl\",\"wb\") as f:\n",
    "    pickle.dump(data_train, f)\n",
    "with open(\"./data_quantized/quant_label_mnist.pkl\",\"wb\") as g:\n",
    "    pickle.dump(label_train, g)\n",
    "    \n",
    "    \n",
    "# Save test data\n",
    "label = testset.test_labels\n",
    "data = testset.test_data\n",
    "\n",
    "data = (data.int()-128)/128\n",
    "data = quant_signed_15(data)\n",
    "data = m(data)\n",
    "\n",
    "with open(\"./data_quantized/quant_test_data_mnist.pkl\",\"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "with open(\"./data_quantized/quant_test_label_mnist.pkl\",\"wb\") as g:\n",
    "    pickle.dump(label, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d70200a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T22:14:26.117110Z",
     "start_time": "2023-02-21T22:14:26.104446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.000,  0.000, -0.500, -0.500, -0.500, -0.500, -0.500, -0.500, -0.125,\n",
       "         0.000, -0.125,  0.000,  0.125,  0.375,  0.375,  0.375,  0.375,  0.375,\n",
       "         0.375,  0.375,  0.375,  0.375,  0.375,  0.000, -0.500, -0.500, -0.500,\n",
       "        -0.500, -0.500, -0.500,  0.000,  0.000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "301a7171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:37:45.984727Z",
     "start_time": "2023-02-15T05:37:45.856727Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/60000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [387]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m label_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(trainset):\n\u001b[1;32m----> 8\u001b[0m     data_a, index_b \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m      9\u001b[0m     data_a \u001b[38;5;241m=\u001b[39m quant_signed_0(data_a)\n\u001b[0;32m     10\u001b[0m     data_list\u001b[38;5;241m.\u001b[39mappend(data_a)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Quantized and save train set\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "data_list = []\n",
    "label_list = []\n",
    "for i in tqdm(trainset):\n",
    "    data_a, index_b = i\n",
    "    data_a = quant_signed_0(data_a)\n",
    "    data_list.append(data_a)\n",
    "    label_list.append(index_b)\n",
    "    \n",
    "with open(\"./data_quantized/quant_data_mnist.pkl\",\"wb\") as f:\n",
    "    pickle.dump(data_list, f)\n",
    "with open(\"./data_quantized/quant_label_mnist.pkl\",\"wb\") as g:\n",
    "    pickle.dump(label_list, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30903046",
   "metadata": {},
   "source": [
    "# Check data well distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "6c41fc98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:58:09.733412Z",
     "start_time": "2023-02-15T05:58:09.683412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -0.97656, -0.85938,\n",
      "         -0.85938, -0.85938, -0.01562,  0.06250,  0.36719, -0.79688,  0.29688,\n",
      "          0.99219,  0.92969, -0.00781, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -0.76562, -0.71875, -0.26562,  0.20312,  0.32812,  0.97656,\n",
      "          0.97656,  0.97656,  0.97656,  0.97656,  0.75781,  0.34375,  0.97656,\n",
      "          0.89062,  0.52344, -0.50000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -0.61719,  0.85938,  0.97656,  0.97656,  0.97656,  0.97656,  0.97656,\n",
      "          0.97656,  0.97656,  0.97656,  0.96094, -0.27344, -0.35938, -0.35938,\n",
      "         -0.56250, -0.69531, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -0.85938,  0.71094,  0.97656,  0.97656,  0.97656,  0.97656,  0.97656,\n",
      "          0.54688,  0.42188,  0.92969,  0.88281, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -0.37500,  0.21875, -0.16406,  0.97656,  0.97656,  0.60156,\n",
      "         -0.91406, -1.00000, -0.66406,  0.20312, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -0.89062, -0.99219,  0.20312,  0.97656, -0.29688,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000,  0.08594,  0.97656,  0.48438,\n",
      "         -0.98438, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -0.91406,  0.48438,  0.97656,\n",
      "         -0.45312, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -0.72656,  0.88281,\n",
      "          0.75781,  0.25000, -0.15625, -0.99219, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -0.36719,\n",
      "          0.87500,  0.97656,  0.97656, -0.07031, -0.80469, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -0.64844,  0.45312,  0.97656,  0.97656,  0.17188, -0.78906, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -0.87500, -0.27344,  0.96875,  0.97656,  0.46094, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000,  0.94531,  0.97656,  0.94531, -0.50000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -0.64062,  0.01562,  0.42969,  0.97656,  0.97656,  0.61719, -0.98438,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -0.69531,  0.15625,\n",
      "          0.78906,  0.97656,  0.97656,  0.97656,  0.95312,  0.42188, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -0.81250, -0.10938,  0.72656,  0.97656,\n",
      "          0.97656,  0.97656,  0.97656,  0.57031, -0.39062, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -0.82031, -0.48438,  0.66406,  0.97656,  0.97656,  0.97656,\n",
      "          0.97656,  0.54688, -0.36719, -0.98438, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -0.85938,\n",
      "          0.33594,  0.71094,  0.97656,  0.97656,  0.97656,  0.97656,  0.52344,\n",
      "         -0.37500, -0.92969, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -0.57031,  0.34375,  0.76562,\n",
      "          0.97656,  0.97656,  0.97656,  0.97656,  0.90625,  0.03906, -0.91406,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000,  0.06250,  0.97656,  0.97656,\n",
      "          0.97656,  0.65625,  0.05469,  0.03125, -0.87500, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000],\n",
      "        [-1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000,\n",
      "         -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000, -1.00000]])\n",
      "tensor([[-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.84375,\n",
      "         -0.84375, -0.84375,  0.00000,  0.06250,  0.34375, -0.78125,  0.28125,\n",
      "          0.96875,  0.90625,  0.00000, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.75000, -0.71875, -0.25000,  0.18750,  0.31250,  0.96875,\n",
      "          0.96875,  0.96875,  0.96875,  0.96875,  0.75000,  0.34375,  0.96875,\n",
      "          0.87500,  0.50000, -0.50000, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.59375,  0.84375,  0.96875,  0.96875,  0.96875,  0.96875,  0.96875,\n",
      "          0.96875,  0.96875,  0.96875,  0.93750, -0.25000, -0.34375, -0.34375,\n",
      "         -0.56250, -0.68750, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.84375,  0.68750,  0.96875,  0.96875,  0.96875,  0.96875,  0.96875,\n",
      "          0.53125,  0.40625,  0.90625,  0.87500, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.37500,  0.21875, -0.15625,  0.96875,  0.96875,  0.59375,\n",
      "         -0.90625, -0.96875, -0.65625,  0.18750, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.87500, -0.96875,  0.18750,  0.96875, -0.28125,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875,  0.06250,  0.96875,  0.46875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.90625,  0.46875,  0.96875,\n",
      "         -0.43750, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.71875,  0.87500,\n",
      "          0.75000,  0.25000, -0.15625, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.34375,\n",
      "          0.87500,  0.96875,  0.96875, -0.06250, -0.78125, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.62500,  0.43750,  0.96875,  0.96875,  0.15625, -0.78125, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.87500, -0.25000,  0.96875,  0.96875,  0.43750, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875,  0.93750,  0.96875,  0.93750, -0.50000,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.62500,  0.00000,  0.40625,  0.96875,  0.96875,  0.59375, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.68750,  0.15625,\n",
      "          0.78125,  0.96875,  0.96875,  0.96875,  0.93750,  0.40625, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.81250, -0.09375,  0.71875,  0.96875,\n",
      "          0.96875,  0.96875,  0.96875,  0.56250, -0.37500, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.81250, -0.46875,  0.65625,  0.96875,  0.96875,  0.96875,\n",
      "          0.96875,  0.53125, -0.34375, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.84375,\n",
      "          0.31250,  0.68750,  0.96875,  0.96875,  0.96875,  0.96875,  0.50000,\n",
      "         -0.37500, -0.90625, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.56250,  0.34375,  0.75000,\n",
      "          0.96875,  0.96875,  0.96875,  0.96875,  0.90625,  0.03125, -0.90625,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875,  0.06250,  0.96875,  0.96875,\n",
      "          0.96875,  0.65625,  0.03125,  0.03125, -0.87500, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875]])\n",
      "tensor([[-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.84375,\n",
      "         -0.84375, -0.84375,  0.00000,  0.06250,  0.34375, -0.78125,  0.28125,\n",
      "          0.96875,  0.90625,  0.00000, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.75000, -0.71875, -0.25000,  0.18750,  0.31250,  0.96875,\n",
      "          0.96875,  0.96875,  0.96875,  0.96875,  0.75000,  0.34375,  0.96875,\n",
      "          0.87500,  0.50000, -0.50000, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.59375,  0.84375,  0.96875,  0.96875,  0.96875,  0.96875,  0.96875,\n",
      "          0.96875,  0.96875,  0.96875,  0.93750, -0.25000, -0.34375, -0.34375,\n",
      "         -0.56250, -0.68750, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.84375,  0.68750,  0.96875,  0.96875,  0.96875,  0.96875,  0.96875,\n",
      "          0.53125,  0.40625,  0.90625,  0.87500, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.37500,  0.21875, -0.15625,  0.96875,  0.96875,  0.59375,\n",
      "         -0.90625, -0.96875, -0.65625,  0.18750, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.87500, -0.96875,  0.18750,  0.96875, -0.28125,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875,  0.06250,  0.96875,  0.46875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.90625,  0.46875,  0.96875,\n",
      "         -0.43750, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.71875,  0.87500,\n",
      "          0.75000,  0.25000, -0.15625, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.34375,\n",
      "          0.87500,  0.96875,  0.96875, -0.06250, -0.78125, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.62500,  0.43750,  0.96875,  0.96875,  0.15625, -0.78125, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.87500, -0.25000,  0.96875,  0.96875,  0.43750, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875,  0.93750,  0.96875,  0.93750, -0.50000,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.62500,  0.00000,  0.40625,  0.96875,  0.96875,  0.59375, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.68750,  0.15625,\n",
      "          0.78125,  0.96875,  0.96875,  0.96875,  0.93750,  0.40625, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.81250, -0.09375,  0.71875,  0.96875,\n",
      "          0.96875,  0.96875,  0.96875,  0.56250, -0.37500, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.81250, -0.46875,  0.65625,  0.96875,  0.96875,  0.96875,\n",
      "          0.96875,  0.53125, -0.34375, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.84375,\n",
      "          0.31250,  0.68750,  0.96875,  0.96875,  0.96875,  0.96875,  0.50000,\n",
      "         -0.37500, -0.90625, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.56250,  0.34375,  0.75000,\n",
      "          0.96875,  0.96875,  0.96875,  0.96875,  0.90625,  0.03125, -0.90625,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875,  0.06250,  0.96875,  0.96875,\n",
      "          0.96875,  0.65625,  0.03125,  0.03125, -0.87500, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
      "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
      "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875]])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True]])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "a = data[0]\n",
    "print(a)\n",
    "print(quant_signed_0(a))\n",
    "print(quant_signed_00(a))\n",
    "print(quant_signed_0(a)==quant_signed_00(a))\n",
    "print(quant_signed_1(a)==quant_signed_11(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813061c",
   "metadata": {},
   "source": [
    "## Check the image after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "60402121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:17:56.501790Z",
     "start_time": "2023-02-15T05:17:56.160408Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2580f5283a0>"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot image\n",
    "plt.imshow(quant_signed_00(trainset[0][0][0]))\n",
    "img = trainset[0]\n",
    "a = img[0].numpy()\n",
    "a = a.transpose(1, 2, 0)\n",
    "plt.imshow(a)\n",
    "\n",
    "plt.imshow(trainset[0][0][0])\n",
    "img = trainset[0]\n",
    "a = img[0].numpy()\n",
    "a = a.transpose(1, 2, 0)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b521e",
   "metadata": {},
   "source": [
    "## Quantizaion and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "9bb1e9ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:40:06.480870Z",
     "start_time": "2023-02-15T05:40:06.235793Z"
    }
   },
   "outputs": [],
   "source": [
    "data_list = quant_signed_0(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "e7fb7aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-15T05:40:20.185019Z",
     "start_time": "2023-02-15T05:40:20.162024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.84375,\n",
       "         -0.84375, -0.84375,  0.00000,  0.06250,  0.34375, -0.78125,  0.28125,\n",
       "          0.96875,  0.90625,  0.00000, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.75000, -0.71875, -0.25000,  0.18750,  0.31250,  0.96875,\n",
       "          0.96875,  0.96875,  0.96875,  0.96875,  0.75000,  0.34375,  0.96875,\n",
       "          0.87500,  0.50000, -0.50000, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.59375,  0.84375,  0.96875,  0.96875,  0.96875,  0.96875,  0.96875,\n",
       "          0.96875,  0.96875,  0.96875,  0.93750, -0.25000, -0.34375, -0.34375,\n",
       "         -0.56250, -0.68750, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.84375,  0.68750,  0.96875,  0.96875,  0.96875,  0.96875,  0.96875,\n",
       "          0.53125,  0.40625,  0.90625,  0.87500, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.37500,  0.21875, -0.15625,  0.96875,  0.96875,  0.59375,\n",
       "         -0.90625, -0.96875, -0.65625,  0.18750, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.87500, -0.96875,  0.18750,  0.96875, -0.28125,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875,  0.06250,  0.96875,  0.46875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.90625,  0.46875,  0.96875,\n",
       "         -0.43750, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.71875,  0.87500,\n",
       "          0.75000,  0.25000, -0.15625, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.34375,\n",
       "          0.87500,  0.96875,  0.96875, -0.06250, -0.78125, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.62500,  0.43750,  0.96875,  0.96875,  0.15625, -0.78125, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.87500, -0.25000,  0.96875,  0.96875,  0.43750, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875,  0.93750,  0.96875,  0.93750, -0.50000,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.62500,  0.00000,  0.40625,  0.96875,  0.96875,  0.59375, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.68750,  0.15625,\n",
       "          0.78125,  0.96875,  0.96875,  0.96875,  0.93750,  0.40625, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.81250, -0.09375,  0.71875,  0.96875,\n",
       "          0.96875,  0.96875,  0.96875,  0.56250, -0.37500, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.81250, -0.46875,  0.65625,  0.96875,  0.96875,  0.96875,\n",
       "          0.96875,  0.53125, -0.34375, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.84375,\n",
       "          0.31250,  0.68750,  0.96875,  0.96875,  0.96875,  0.96875,  0.50000,\n",
       "         -0.37500, -0.90625, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.56250,  0.34375,  0.75000,\n",
       "          0.96875,  0.96875,  0.96875,  0.96875,  0.90625,  0.03125, -0.90625,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875,  0.06250,  0.96875,  0.96875,\n",
       "          0.96875,  0.65625,  0.03125,  0.03125, -0.87500, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875],\n",
       "        [-0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875,\n",
       "         -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875, -0.96875]])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data_quantized/quant_data_mnist.pkl\",\"wb\") as f:\n",
    "    pickle.dump(data_list, f)\n",
    "with open(\"./data_quantized/quant_label_mnist.pkl\",\"wb\") as g:\n",
    "    pickle.dump(label_list, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "049a4028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T20:14:36.204800Z",
     "start_time": "2023-02-12T20:10:42.294111Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10000/10000 [03:52<00:00, 43.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Quantized and save test set\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "data_list = []\n",
    "label_list = []\n",
    "for i in tqdm(testset):\n",
    "    data_a, index_b = i\n",
    "    data_a = quant_signed_0(data_a)\n",
    "    data_list.append(data_a)\n",
    "    label_list.append(index_b)\n",
    "    \n",
    "with open(\"./data_quantized/quant_test_data.pkl\",\"wb\") as w:\n",
    "    pickle.dump(data_list, w)\n",
    "    \n",
    "with open(\"./data_quantized/quant_test_label.pkl\",\"wb\") as k:\n",
    "    pickle.dump(label_list, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c523dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e7dc675",
   "metadata": {},
   "source": [
    "# Next level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "3ace23d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T21:10:45.043292Z",
     "start_time": "2023-02-12T21:10:45.037436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name '' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "except Exception  as e: \n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "id": "e6e90a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T21:31:19.755781Z",
     "start_time": "2023-02-12T21:31:19.733646Z"
    }
   },
   "outputs": [],
   "source": [
    "file  = open('./latest.txt' , 'w' )\n",
    "checkpoint_dir = '/123213/'\n",
    "epoch = 100\n",
    "file.write(os.path.join(checkpoint_dir, 'checkpoint_{}.tar'.format(epoch))) \n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "id": "4124cb23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T21:21:30.721132Z",
     "start_time": "2023-02-12T21:21:30.697511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 12 16 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2_12_Time_16_21'"
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon = time.localtime().tm_mon\n",
    "day = time.localtime().tm_mday\n",
    "hour = time.localtime().tm_hour\n",
    "minu = time.localtime().tm_min\n",
    "\n",
    "\n",
    "mon = str(mon)\n",
    "day = str(day)\n",
    "hour = str(hour)\n",
    "minu = str(minu)\n",
    "now = mon + \"_\" + day + \"_Time_\" + hour + \"_\" + minu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23feb2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "b81824be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T21:19:15.624461Z",
     "start_time": "2023-02-12T21:19:15.608884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sun Feb 12 16:19:15 2023'"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time.ctime()\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "4a72bfc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T21:18:46.985513Z",
     "start_time": "2023-02-12T21:18:46.965853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feb_12_16_18'"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = now.replace(\":\",\"_\")\n",
    "now = now.replace(\" \",\"_\")\n",
    "now[4:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "da446e3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:34:55.782915Z",
     "start_time": "2023-02-12T06:34:55.754145Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [600]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m b \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(data_list,\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mtensor(a)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "a = random.sample(label_list,256)\n",
    "b = random.sample(data_list,256)\n",
    "torch.tensor(a)\n",
    "torch.tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "dd1f3da6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:43:02.033062Z",
     "start_time": "2023-02-12T06:43:01.870238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000],\n",
       "        [0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000],\n",
       "        [0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000],\n",
       "        ...,\n",
       "        [0.56250, 0.56250, 0.56250,  ..., 0.00000, 0.00000, 0.00000],\n",
       "        [0.53125, 0.56250, 0.59375,  ..., 0.00000, 0.00000, 0.00000],\n",
       "        [0.53125, 0.53125, 0.53125,  ..., 0.00000, 0.00000, 0.00000]])"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran = random.sample(range(50000),256)\n",
    "a = []\n",
    "b = torch.tensor([])\n",
    "for i in ran:\n",
    "    a = data_list[i]\n",
    "    b = torch.cat((b,a),0)\n",
    "b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "1726fde5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:45:24.625258Z",
     "start_time": "2023-02-12T06:45:24.566826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.cat((random.sample(data_list,256)),0)\n",
    "b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "d0f9a812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:45:07.474007Z",
     "start_time": "2023-02-12T06:45:07.465833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = random.sample(data_list,256)\n",
    "d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "3d731829",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:47:38.462581Z",
     "start_time": "2023-02-12T06:47:38.440581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00000, 0.00000, 0.00000,  ..., 0.65625, 0.65625, 0.65625],\n",
       "        [0.00000, 0.00000, 0.00000,  ..., 0.65625, 0.65625, 0.65625],\n",
       "        [0.00000, 0.00000, 0.00000,  ..., 0.65625, 0.65625, 0.65625],\n",
       "        ...,\n",
       "        [0.00000, 0.00000, 0.00000,  ..., 0.15625, 0.15625, 0.15625],\n",
       "        [0.00000, 0.00000, 0.00000,  ..., 0.18750, 0.15625, 0.15625],\n",
       "        [0.00000, 0.00000, 0.00000,  ..., 0.18750, 0.18750, 0.15625]])"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = d[0]\n",
    "torch.cat((d[0],d[1]),1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fa9893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4026bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "f2b31ef1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:36:09.622285Z",
     "start_time": "2023-02-12T06:36:09.602291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.41399],\n",
       "        [0.35471],\n",
       "        [0.62400]])"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "2598c6a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:34:43.156830Z",
     "start_time": "2023-02-12T06:34:43.132748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "torch.tensor([a,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "155eada8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:51:21.851507Z",
     "start_time": "2023-02-12T06:51:21.845507Z"
    }
   },
   "outputs": [],
   "source": [
    "a = d[0]\n",
    "b = d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "517df496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:53:44.458732Z",
     "start_time": "2023-02-12T06:53:44.451632Z"
    }
   },
   "outputs": [],
   "source": [
    "c = [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "2adb3fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:53:55.602661Z",
     "start_time": "2023-02-12T06:53:55.590333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 32, 32])"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.stack(c)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "21f7ff61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:52:17.070436Z",
     "start_time": "2023-02-12T06:52:17.060932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "b7756d4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:52:35.685563Z",
     "start_time": "2023-02-12T06:52:35.668711Z"
    }
   },
   "outputs": [],
   "source": [
    "my_tensor = torch.cat([a, b], dim=-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "a5a581d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:52:35.903976Z",
     "start_time": "2023-02-12T06:52:35.892005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "28fad11e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T06:54:27.393160Z",
     "start_time": "2023-02-12T06:54:27.338161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 32, 32])"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.stack(random.sample(data_list,256))\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "e20835bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T07:01:39.432282Z",
     "start_time": "2023-02-12T07:01:39.408208Z"
    }
   },
   "outputs": [],
   "source": [
    "num = random.sample(range(50000),256)\n",
    "data_num = []\n",
    "label_num = []\n",
    "for i in num:\n",
    "    data_num.append(data_list[i])\n",
    "    label_num.append(label_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "884738d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T19:17:22.330677Z",
     "start_time": "2023-02-12T19:17:22.304672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.46875, 0.56250, 0.37500,  ..., 0.90625, 0.00000, 0.00000],\n",
       "         [0.34375, 0.34375, 0.31250,  ..., 0.90625, 0.00000, 0.00000],\n",
       "         ...,\n",
       "         [0.53125, 0.53125, 0.50000,  ..., 0.43750, 0.00000, 0.00000],\n",
       "         [0.50000, 0.50000, 0.53125,  ..., 0.46875, 0.00000, 0.00000],\n",
       "         [0.53125, 0.56250, 0.56250,  ..., 0.43750, 0.00000, 0.00000]],\n",
       "\n",
       "        [[0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.50000, 0.59375, 0.40625,  ..., 0.90625, 0.00000, 0.00000],\n",
       "         [0.34375, 0.37500, 0.34375,  ..., 0.90625, 0.00000, 0.00000],\n",
       "         ...,\n",
       "         [0.53125, 0.50000, 0.50000,  ..., 0.37500, 0.00000, 0.00000],\n",
       "         [0.50000, 0.50000, 0.50000,  ..., 0.40625, 0.00000, 0.00000],\n",
       "         [0.50000, 0.50000, 0.50000,  ..., 0.37500, 0.00000, 0.00000]],\n",
       "\n",
       "        [[0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.46875, 0.53125, 0.34375,  ..., 0.93750, 0.00000, 0.00000],\n",
       "         [0.31250, 0.34375, 0.31250,  ..., 0.90625, 0.00000, 0.00000],\n",
       "         ...,\n",
       "         [0.18750, 0.18750, 0.15625,  ..., 0.15625, 0.00000, 0.00000],\n",
       "         [0.18750, 0.18750, 0.18750,  ..., 0.18750, 0.00000, 0.00000],\n",
       "         [0.18750, 0.25000, 0.25000,  ..., 0.15625, 0.00000, 0.00000]]])"
      ]
     },
     "execution_count": 762,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "dfbfcdb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T08:58:08.419735Z",
     "start_time": "2023-02-12T08:58:08.409422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sun_Feb_12_03'"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time.ctime()\n",
    "now = now.replace(\":\",\"_\")\n",
    "now = now.replace(\" \",\"_\")\n",
    "now[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "65bfa453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T09:06:30.273504Z",
     "start_time": "2023-02-12T09:06:30.253504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divmod(18,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "961b9b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T18:11:31.842175Z",
     "start_time": "2023-02-12T18:11:31.824172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "a = 0.043\n",
    "print(\"%.f\" % (a*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "790b10ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T09:00:05.671468Z",
     "start_time": "2023-02-12T09:00:05.656468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sun Feb 12 04'"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.ctime()[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "3db7d3e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T08:24:11.035667Z",
     "start_time": "2023-02-12T08:24:11.025762Z"
    }
   },
   "outputs": [],
   "source": [
    "a = str(a)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "d618ac05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T08:24:13.871171Z",
     "start_time": "2023-02-12T08:24:13.862526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23'"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "4d2bb958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T08:23:59.190070Z",
     "start_time": "2023-02-12T08:23:59.172930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23'"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "7c45e980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T05:47:51.008719Z",
     "start_time": "2023-02-12T05:47:50.976019Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./quant_data.pkl\",\"rb\") as f:\n",
    "    data_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "bb118ec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T05:47:53.525045Z",
     "start_time": "2023-02-12T05:47:53.517943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "cebe6c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T05:12:10.541979Z",
     "start_time": "2023-02-12T05:12:09.213376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cifar10 data ... \n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "val_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/tmpssd/pabillam/data', train=False, download=True, transform=val_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=False, num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ee9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trainset)):\n",
    "    trainset[i][0] = quant_signed_0(trainset[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "11952703",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T05:12:22.646062Z",
     "start_time": "2023-02-12T05:12:22.623615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15., 15., 16.,  ...,  0.,  0.,  0.],\n",
       "         [16., 16., 17.,  ...,  0.,  0.,  0.],\n",
       "         [15., 16., 17.,  ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 2.,  2.,  6.,  ...,  0.,  0.,  0.],\n",
       "         [ 2.,  5., 14.,  ...,  0.,  0.,  0.],\n",
       "         [ 3.,  7., 17.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[22., 22., 22.,  ...,  0.,  0.,  0.],\n",
       "         [22., 23., 23.,  ...,  0.,  0.,  0.],\n",
       "         [22., 23., 23.,  ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [10.,  9., 11.,  ...,  0.,  0.,  0.],\n",
       "         [ 9., 11., 19.,  ...,  0.,  0.,  0.],\n",
       "         [10., 13., 22.,  ...,  0.,  0.,  0.]],\n",
       "\n",
       "        [[27., 27., 27.,  ...,  0.,  0.,  0.],\n",
       "         [28., 28., 28.,  ...,  0.,  0.,  0.],\n",
       "         [27., 27., 28.,  ...,  0.,  0.,  0.],\n",
       "         ...,\n",
       "         [14., 12., 14.,  ...,  0.,  0.,  0.],\n",
       "         [14., 15., 22.,  ...,  0.,  0.,  0.],\n",
       "         [14., 18., 26.,  ...,  0.,  0.,  0.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "20209343",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T04:57:27.659111Z",
     "start_time": "2023-02-12T04:57:27.634935Z"
    }
   },
   "outputs": [],
   "source": [
    "b = quant_signed_0(trainset[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "9b8dd5e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T04:59:38.667914Z",
     "start_time": "2023-02-12T04:59:38.651224Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset2 = trainset1\n",
    "trainset1 = trainset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "a72ba981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T04:57:37.018172Z",
     "start_time": "2023-02-12T04:57:36.994670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.59375, 0.46875, 0.40625,  ..., 0.34375, 0.31250, 0.28125],\n",
       "         [0.53125, 0.56250, 0.46875,  ..., 0.37500, 0.28125, 0.25000],\n",
       "         [0.53125, 0.53125, 0.43750,  ..., 0.28125, 0.25000, 0.25000],\n",
       "         ...,\n",
       "         [0.65625, 0.59375, 0.59375,  ..., 0.15625, 0.21875, 0.34375],\n",
       "         [0.62500, 0.59375, 0.59375,  ..., 0.37500, 0.46875, 0.50000],\n",
       "         [0.62500, 0.59375, 0.62500,  ..., 0.53125, 0.53125, 0.53125]],\n",
       "\n",
       "        [[0.68750, 0.53125, 0.40625,  ..., 0.34375, 0.34375, 0.31250],\n",
       "         [0.62500, 0.59375, 0.46875,  ..., 0.37500, 0.31250, 0.28125],\n",
       "         [0.59375, 0.56250, 0.43750,  ..., 0.31250, 0.25000, 0.25000],\n",
       "         ...,\n",
       "         [0.62500, 0.59375, 0.62500,  ..., 0.12500, 0.18750, 0.31250],\n",
       "         [0.59375, 0.59375, 0.62500,  ..., 0.34375, 0.43750, 0.46875],\n",
       "         [0.56250, 0.56250, 0.59375,  ..., 0.50000, 0.50000, 0.50000]],\n",
       "\n",
       "        [[0.71875, 0.53125, 0.34375,  ..., 0.25000, 0.25000, 0.25000],\n",
       "         [0.65625, 0.59375, 0.43750,  ..., 0.28125, 0.21875, 0.21875],\n",
       "         [0.62500, 0.56250, 0.43750,  ..., 0.25000, 0.18750, 0.18750],\n",
       "         ...,\n",
       "         [0.62500, 0.62500, 0.65625,  ..., 0.12500, 0.21875, 0.34375],\n",
       "         [0.50000, 0.50000, 0.53125,  ..., 0.37500, 0.46875, 0.50000],\n",
       "         [0.46875, 0.46875, 0.50000,  ..., 0.53125, 0.53125, 0.56250]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_signed_0(trainset[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "45b10def",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T04:58:35.369033Z",
     "start_time": "2023-02-12T04:58:35.338960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.59375, 0.46875, 0.40625,  ..., 0.34375, 0.31250, 0.28125],\n",
       "         [0.53125, 0.56250, 0.46875,  ..., 0.37500, 0.28125, 0.25000],\n",
       "         [0.53125, 0.53125, 0.43750,  ..., 0.28125, 0.25000, 0.25000],\n",
       "         ...,\n",
       "         [0.65625, 0.59375, 0.59375,  ..., 0.15625, 0.21875, 0.34375],\n",
       "         [0.62500, 0.59375, 0.59375,  ..., 0.37500, 0.46875, 0.50000],\n",
       "         [0.62500, 0.59375, 0.62500,  ..., 0.53125, 0.53125, 0.53125]],\n",
       "\n",
       "        [[0.68750, 0.53125, 0.40625,  ..., 0.34375, 0.34375, 0.31250],\n",
       "         [0.62500, 0.59375, 0.46875,  ..., 0.37500, 0.31250, 0.28125],\n",
       "         [0.59375, 0.56250, 0.43750,  ..., 0.31250, 0.25000, 0.25000],\n",
       "         ...,\n",
       "         [0.62500, 0.59375, 0.62500,  ..., 0.12500, 0.18750, 0.31250],\n",
       "         [0.59375, 0.59375, 0.62500,  ..., 0.34375, 0.43750, 0.46875],\n",
       "         [0.56250, 0.56250, 0.59375,  ..., 0.50000, 0.50000, 0.50000]],\n",
       "\n",
       "        [[0.71875, 0.53125, 0.34375,  ..., 0.25000, 0.25000, 0.25000],\n",
       "         [0.65625, 0.59375, 0.43750,  ..., 0.28125, 0.21875, 0.21875],\n",
       "         [0.62500, 0.56250, 0.43750,  ..., 0.25000, 0.18750, 0.18750],\n",
       "         ...,\n",
       "         [0.62500, 0.62500, 0.65625,  ..., 0.12500, 0.21875, 0.34375],\n",
       "         [0.50000, 0.50000, 0.53125,  ..., 0.37500, 0.46875, 0.50000],\n",
       "         [0.46875, 0.46875, 0.50000,  ..., 0.53125, 0.53125, 0.56250]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = quant_signed_0(trainset[1][0])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5d2f508b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T04:52:57.240114Z",
     "start_time": "2023-02-12T04:52:57.227718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.35294, 0.32549, 0.25490,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.34510, 0.38039, 0.27059,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.27059, 0.36863, 0.35294,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         ...,\n",
       "         [0.54902, 0.52549, 0.45490,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000]],\n",
       "\n",
       "        [[0.36471, 0.33725, 0.26275,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.35686, 0.39216, 0.28235,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.28627, 0.38039, 0.36471,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         ...,\n",
       "         [0.50980, 0.49804, 0.44314,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000]],\n",
       "\n",
       "        [[0.29412, 0.27451, 0.21176,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.27451, 0.31373, 0.21569,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.19608, 0.29020, 0.28627,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         ...,\n",
       "         [0.53333, 0.52157, 0.47843,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000],\n",
       "         [0.00000, 0.00000, 0.00000,  ..., 0.00000, 0.00000, 0.00000]]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6de45701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T03:08:41.694811Z",
     "start_time": "2023-02-12T03:08:41.683462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train_checkpoints/CNN_627_cifar10/checkpoint_100.tar\n",
      "./quant_cnn_checkpoints/CNN_627_cifar10/\n"
     ]
    }
   ],
   "source": [
    "model = nets.CNN_627()\n",
    "pretrained_weights = './train_checkpoints/{}_{}/checkpoint_100.tar'.format(network, dataset)\n",
    "checkpoint_dir = './quant_cnn_checkpoints/{}_{}/'.format(network, dataset)\n",
    "batch_size = 32\n",
    "retraining_epochs = 30\n",
    "base_lr = 0.001\n",
    "weight_decay = 1e-4\n",
    "log_path = './logs/{}_{}/quant_cnn.log'.format(network, dataset)\n",
    "\n",
    "print(pretrained_weights)\n",
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d19cbbea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T03:08:42.428069Z",
     "start_time": "2023-02-12T03:08:42.400934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint './train_checkpoints/CNN_627_cifar10/checkpoint_100.tar'\n",
      "dict_keys(['epoch', 'state_dict'])\n",
      "odict_keys(['conv1_1.weight', 'conv1_1.bias', 'conv2_1.weight', 'conv2_1.bias', 'conv3_1.weight', 'conv3_1.bias', 'conv4_1.weight', 'conv4_1.bias', 'fc6.weight', 'fc6.bias', 'fc7.weight', 'fc7.bias'])\n"
     ]
    }
   ],
   "source": [
    "# load pretrained checkpoint\n",
    "print(\"Loading checkpoint '{}'\".format(pretrained_weights))\n",
    "pretrained_ckpt = torch.load(pretrained_weights)\n",
    "print(pretrained_ckpt.keys())\n",
    "print(pretrained_ckpt['state_dict'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "830a3ffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T03:08:44.727444Z",
     "start_time": "2023-02-12T03:08:44.717442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint './train_checkpoints/CNN_627_cifar10/checkpoint_100.tar'\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(pretrained_ckpt['state_dict'])\n",
    "print(\"Loaded checkpoint '{}'\".format(pretrained_weights))\n",
    "\n",
    "# setup checkpoint directory\n",
    "if not pth.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3f091ad8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T03:09:06.674432Z",
     "start_time": "2023-02-12T03:08:52.908934Z"
    }
   },
   "outputs": [],
   "source": [
    "log = tools.StatLogger(log_path)\n",
    "\n",
    "# ===================================\n",
    "# initialize and run training session\n",
    "# ===================================\n",
    "model.cuda()\n",
    "lossfunc = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "# retrive pruning masks\n",
    "weight_masks = retrieve_masks(model)\n",
    "\n",
    "# Apply quantization\n",
    "index = 0\n",
    "for n, m in model.named_modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        if m.kernel_size == (3, 3):\n",
    "            m.weight.data = quant_signed_1(m.weight.data, 6)\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1f1e17ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T03:09:09.094537Z",
     "start_time": "2023-02-12T03:09:09.084945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of CNN_627(\n",
       "  (conv1_1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc6): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc7): Linear(in_features=64, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "de158854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T03:09:09.862114Z",
     "start_time": "2023-02-12T03:09:09.851119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_627(\n",
       "  (conv1_1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc6): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc7): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc79ca56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T03:10:02.217951Z",
     "start_time": "2023-02-12T03:10:02.209942Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "efc519db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T03:11:18.094103Z",
     "start_time": "2023-02-12T03:11:17.948110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2553cc16490>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.flatten(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0af8899f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T03:09:17.281556Z",
     "start_time": "2023-02-12T03:09:13.497420Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.DoubleTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossfunc(outputs, labels)\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\cnn\\cnn_quant\\cnn_cifar10\\nets.py:191\u001b[0m, in \u001b[0;36mCNN_627.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 191\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    192\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x)\n\u001b[0;32m    193\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_1(x))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.DoubleTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# Retraining steps\n",
    "for epoch in range(retraining_epochs):\n",
    "    epoch += 1\n",
    "    model.train()\n",
    "    error_top1 = []\n",
    "    error_top5 = []\n",
    "    running_loss = []\n",
    "\n",
    "    for idx, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = lossfunc(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # get masked weights\n",
    "        apply_mask(model, weight_masks)\n",
    "\n",
    "        error_top1.append(tools.topK_error(outputs, labels, K=1).item())\n",
    "        error_top5.append(tools.topK_error(outputs, labels, K=5).item())\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "    error_top1 = np.average(error_top1)\n",
    "    error_top5 = np.average(error_top5)\n",
    "    running_loss = np.average(running_loss)\n",
    "    # print statistics\n",
    "    print(\"RETRAIN epoch:%-4d error_top1: %.4f error_top5: %.4f loss:%.4f\" % (\n",
    "        epoch, error_top1, error_top5, running_loss))\n",
    "    log.report(epoch=epoch,\n",
    "               split='RETRAIN',\n",
    "               error_top5=float(error_top5),\n",
    "               error_top1=float(error_top1),\n",
    "               loss=float(running_loss))\n",
    "\n",
    "    # Quantize again\n",
    "    index = 0\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if m.kernel_size == (3, 3):\n",
    "                m.weight.data = quant_signed_1(m.weight.data, 6)\n",
    "                index += 1\n",
    "\n",
    "    validate(model, testloader, lossfunc, log, epoch)\n",
    "\n",
    "    print('-- saving model check point')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "    }, os.path.join(checkpoint_dir, 'checkpoint_{}.tar'.format(epoch)))\n",
    "\n",
    "print('Finished Retraining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "dbf768bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T04:45:01.242470Z",
     "start_time": "2023-02-12T04:45:01.235489Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7114da7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T04:46:58.928884Z",
     "start_time": "2023-02-12T04:46:58.912150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sat Feb 11 23_46_58 2023'"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time.ctime()\n",
    "now.replace(\":\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e07237cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-12T04:47:09.742275Z",
     "start_time": "2023-02-12T04:47:09.734299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sat Feb 11 23:46:58 2023'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2654be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298.75px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "358.828px",
    "left": "1042.05px",
    "right": "20px",
    "top": "729.969px",
    "width": "356.172px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c90298c6112196fbfa1fbb23f13f4a1aa53ac151450eb01d8b17d4ea5324d4aa"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
