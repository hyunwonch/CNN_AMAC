{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfac8f03",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673be2bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T23:16:30.651588Z",
     "start_time": "2023-04-03T23:16:30.631170Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Library\n",
    "import sys\n",
    "import os\n",
    "import os.path as pth\n",
    "\n",
    "#!pip install torchplot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import nets\n",
    "import datasets\n",
    "import tools\n",
    "import layers as L\n",
    "import train\n",
    "\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from slacker import Slacker\n",
    "from quantization import *\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200e405",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9710fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T23:16:32.659592Z",
     "start_time": "2023-04-03T23:16:32.649238Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Custom functions - Completed\n",
    "\n",
    "@jit\n",
    "def amac(x, w):\n",
    "    return (x * w).sum()\n",
    "\n",
    "\n",
    "@jit\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "\n",
    "@jit\n",
    "def padding(a):\n",
    "    result = np.zeros((a.shape[0],a.shape[1]+2,a.shape[2]+2))\n",
    "    for i in range(a.shape[0]):\n",
    "        result[i] = np.pad(a[i],1)\n",
    "    return result\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def maxpooling(x_original):\n",
    "   \n",
    "    depth = x_original.shape[0] \n",
    "    row = int((x_original.shape[1])/2)\n",
    "    col = int((x_original.shape[2])/2)\n",
    "\n",
    "    one_layer = np.zeros((depth,row,col))\n",
    "    \n",
    "    for d in range(depth):\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                r = x_original[d,2*i:2*i+2,2*j:2*j+2].max()\n",
    "                one_layer[d,i,j] = r\n",
    "    return one_layer\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def quant_signed_05_np(original, bit=5):\n",
    "    bit = bit -1\n",
    "    original = np.clip(original, -0.9375, 0.9375)\n",
    "    original = original * (2**bit)\n",
    "    \n",
    "    (row, col) = original.shape\n",
    "    result = np.zeros((row,col))\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            result[i,j] = math.trunc(original[i,j])/ (2**bit)\n",
    "    return result\n",
    "\n",
    "@jit\n",
    "def quant_signed_05_np_fc(original, bit=5):\n",
    "    bit = bit -1\n",
    "    original = np.clip(original, -0.9375, 0.9375)\n",
    "    original = original * (2**bit)\n",
    "    \n",
    "    result = math.trunc(original)/ (2**bit)\n",
    "    return result\n",
    "\n",
    "\n",
    "get_bin = lambda x, n: format(x, 'b').zfill(n).replace(\"-\",\"1\")\n",
    "\n",
    "# Partial sum function with point parameters - Completed\n",
    "\n",
    "# This is partial sum function for convolution layer -> Matches with verilog\n",
    "@jit(cache=True)\n",
    "def partial_sum_fa_conv_point(original, bit=5, point = 1):\n",
    "    a = original\n",
    "    bit = bit - 2  + (point - 1)\n",
    "    value = 1.875/(2**(point-1))\n",
    "    \n",
    "    result = np.zeros(a.shape[0])\n",
    "    for d in range(a.shape[0]):\n",
    "        partial = a[d].sum()\n",
    "        if partial > value:\n",
    "            partial = value\n",
    "        elif partial < -value:\n",
    "            partial = -value\n",
    "        else:\n",
    "            partial = partial\n",
    "        result[d] = math.trunc(partial* (2**bit)) / (2**bit)\n",
    "    return result.sum()\n",
    "\n",
    "# This function matches with our verilog model & reasonable accuracy\n",
    "@jit(cache=True)\n",
    "def partial_sum_fa_fc_point(original, bit=5, point=1):\n",
    "    a = original\n",
    "    bit = bit - 2  + (point - 1)\n",
    "    value = 1.875/(2**(point-1))\n",
    "    \n",
    "    partial = a.sum()\n",
    "    if partial > value:\n",
    "        partial = value\n",
    "    elif partial < -value:\n",
    "        partial = -value\n",
    "    else:\n",
    "        partial = partial\n",
    "    result= math.trunc(partial* (2**bit)) / (2**bit)\n",
    "    return result\n",
    "\n",
    "# FC & Conv function with point parameters - completed\n",
    "# These functions have same fixed point with bias & match with verilog file\n",
    "@jit(cache=True)\n",
    "def fc_fa_05(x_original, w, b, p=1):\n",
    "   \n",
    "    filt = w.shape[0]\n",
    "    stage = int(x_original.shape[1]/8)\n",
    "    c = np.zeros((1,filt))\n",
    "    for f in range(filt):\n",
    "        re = 0\n",
    "        for i in range(stage):\n",
    "            r = x_original[0,i*8:i*8+8] * w[f,i*8:i*8+8]\n",
    "            re = re + partial_sum_fa_fc_point(r, point=p)\n",
    "        c[0,f] = quant_signed_05_np_fc(re + b[f])\n",
    "    return c\n",
    "\n",
    "@jit(cache=True)\n",
    "def conv_custom_fa_05(x_original, w, b, p=1):\n",
    "   \n",
    "    filt = w.shape[0]\n",
    "    depth = x_original.shape[0] \n",
    "    row = x_original.shape[1] - 2\n",
    "    col = x_original.shape[2] - 2\n",
    "\n",
    "    c = np.zeros((filt,row,col))\n",
    "    one_layer = np.zeros((row,col))\n",
    "    \n",
    "    for f in range(filt):\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                r = x_original[:,i:i+3,j:j+3] * w[f]\n",
    "                one_layer[i,j] = partial_sum_fa_conv_point(r, point=p)\n",
    "        c[f,:,:] = quant_signed_05_np(one_layer + b[f])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed91364b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T23:16:37.161143Z",
     "start_time": "2023-04-03T23:16:37.142141Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Function of saving parameter\n",
    "def save_conv_weight(weight, bias, file_name, path):\n",
    "    if(type(path) != str):\n",
    "        path = str(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory {path} created.\")\n",
    "    # else:\n",
    "    #     print(f\"Directory {path} already exists.\")\n",
    "    if(type(file_name) != str):\n",
    "        path = str(file_name)\n",
    "    if \".txt\" not in file_name:\n",
    "        weight_name = file_name + \"_weight.txt\"\n",
    "        bias_name = file_name + \"_bias.txt\"\n",
    "    file = open(path + \"/\" + weight_name,'w')\n",
    "\n",
    "    a = weight\n",
    "    a = a * 16\n",
    "    for d in range(a.shape[1]):\n",
    "        for f in range(a.shape[0]):\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    file.write(get_bin(int(a[f,d,i,j]),5)+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    file = open(path + \"/\" + bias_name,'w')\n",
    "    a = bias\n",
    "    a = a * 16\n",
    "    for i in range(a.shape[0]):\n",
    "        file.write(get_bin(int(a[i]),5)+\"\\n\")\n",
    "    file.close()\n",
    "    print(f\"Save {file_name} weight to {path}/{weight_name}\\nSave {file_name} bias   to {path}/{bias_name}\")\n",
    "\n",
    "def save_fc_weight(weight, bias, file_name, path, include_bias = 0):\n",
    "    if(type(path) != str):\n",
    "        path = str(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory {path} created.\")\n",
    "    # else:\n",
    "    #     print(f\"Directory {path} already exists.\")\n",
    "    if(type(file_name) != str):\n",
    "        path = str(file_name)\n",
    "    if \".txt\" not in file_name:\n",
    "        file_name = file_name + \".txt\"\n",
    "    \n",
    "    file = open(path + \"/\" + file_name,'w')\n",
    "\n",
    "    weight = weight*16\n",
    "    bias = bias * 16\n",
    "    for w in range(0, weight.shape[0], 32):\n",
    "        for mac in range(min(weight.shape[0] - w, 32)):\n",
    "            if include_bias:\n",
    "                file.write(get_bin(int(bias[w+mac]),5)+\"\\n\")\n",
    "            else:\n",
    "                file.write(\"0\\n\")\n",
    "        for z in range(0, weight.shape[1], 8):\n",
    "            for mac in range(min(weight.shape[0] - w, 32)):\n",
    "                for port in range(8):\n",
    "                    file.write(get_bin(int(weight[w+mac,z+port]),5)+\"\\n\")\n",
    "                    \n",
    "    file.close()\n",
    "    print(f\"Save {file_name[:-4]} weight to {path}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a6652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_conv_result(x, file_name, path):\n",
    "    if(type(path) != str):\n",
    "        path = str(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory {path} created.\")\n",
    "    # else:\n",
    "    #     print(f\"Directory {path} already exists.\")\n",
    "    if(type(file_name) != str):\n",
    "        path = str(file_name)\n",
    "    if \".txt\" not in file_name:\n",
    "        output_name = file_name + \"_output.txt\"\n",
    "    file = open(path + \"/\" + output_name,'w')\n",
    "\n",
    "    save = x*16\n",
    "    for d in range(x.shape[0]):\n",
    "        for i in range(x.shape[1]):\n",
    "            for j in range(x.shape[2]):\n",
    "                file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96a068",
   "metadata": {},
   "source": [
    "# Open model and file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b1d12e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T23:16:40.195927Z",
     "start_time": "2023-04-03T23:16:40.088005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quant MNIST\n",
      "########## Loaded checkpoint './checkpoints_quant/mnist_quant_mnist/2_18_Time_16_36/checkpoint_7_98.3.tar'\n",
      "mnist dataset with different fixed point\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and data\n",
    "\n",
    "file = open('latest.txt' , 'r' )\n",
    "line = file.readline()\n",
    "pretrained_checkpoint = line\n",
    "file.close()\n",
    "\n",
    "# Default settings for arch, dataset, and checkpoint\n",
    "arch = \"CNN_627_large\"\n",
    "dataset = \"cifar10\"\n",
    "batch_size = 256\n",
    "pretrained_checkpoint = pretrained_checkpoint\n",
    "\n",
    "# trainloader, _, testloader = datasets.get_mnist(batch_size)\n",
    "model = nets.mnist_quant()\n",
    "\n",
    "pretrained_ckpt = torch.load(pretrained_checkpoint)\n",
    "model.load_state_dict(pretrained_ckpt['state_dict'])\n",
    "print(\"########## Loaded checkpoint '{}'\".format(pretrained_checkpoint))\n",
    "\n",
    "print(\"mnist dataset with different fixed point\")\n",
    "with open(\"./data_quantized/quant_test_data_mnist.pkl\",\"rb\") as f:\n",
    "    data_list = pickle.load(f)\n",
    "with open(\"./data_quantized/quant_test_label_mnist.pkl\",\"rb\") as g:\n",
    "    label_list = pickle.load(g)\n",
    "    \n",
    "\n",
    "# print(\"mnist dataset with same fixed point\")\n",
    "# with open(\"./data_quantized/quant_test_data_mnist_05.pkl\",\"rb\") as f:\n",
    "#     data_list2 = pickle.load(f)\n",
    "# with open(\"./data_quantized/quant_test_label_mnist_05.pkl\",\"rb\") as g:\n",
    "#     label_list2 = pickle.load(g)\n",
    "    \n",
    "# Load Weights and biases\n",
    "\n",
    "w1, w2, w3, w4, w5, w6 = model.conv1.weight, model.conv2.weight, model.conv3.weight, model.conv4.weight, model.fc5.weight, model.fc6.weight\n",
    "b1, b2, b3, b4, b5, b6 = model.conv1.bias, model.conv2.bias, model.conv3.bias, model.conv4.bias, model.fc5.bias, model.fc6.bias\n",
    "\n",
    "w1 = w1.data.numpy()\n",
    "b1 = b1.data.numpy()\n",
    "\n",
    "w2 = w2.data.numpy()\n",
    "b2 = b2.data.numpy()\n",
    "\n",
    "w3 = w3.data.numpy()\n",
    "b3 = b3.data.numpy()\n",
    "\n",
    "w4 = w4.data.numpy()\n",
    "b4 = b4.data.numpy()\n",
    "\n",
    "w5 = w5.data.numpy()\n",
    "b5 = b5.data.numpy()\n",
    "\n",
    "w6 = w6.data.numpy()\n",
    "b6 = b6.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f98327e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T23:12:00.455735Z",
     "start_time": "2023-04-03T23:12:00.040709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save conv1 weight to weight_new/conv1_weight.txt\n",
      "Save conv1 bias   to weight_new/conv1_bias.txt\n",
      "Save conv2 weight to weight_new/conv2_weight.txt\n",
      "Save conv2 bias   to weight_new/conv2_bias.txt\n",
      "Save conv3 weight to weight_new/conv3_weight.txt\n",
      "Save conv3 bias   to weight_new/conv3_bias.txt\n",
      "Save conv4 weight to weight_new/conv4_weight.txt\n",
      "Save conv4 bias   to weight_new/conv4_bias.txt\n",
      "Save fc5 weight to weight_new/fc5.txt\n",
      "Save fc6 weight to weight_new/fc6.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"weight_new\"\n",
    "\n",
    "save_conv_weight(w1, b1, \"conv1\", path)\n",
    "save_conv_weight(w2, b2, \"conv2\", path)\n",
    "save_conv_weight(w3, b3, \"conv3\", path)\n",
    "save_conv_weight(w4, b4, \"conv4\", path)\n",
    "save_fc_weight(w5, b5, \"fc5\", path)\n",
    "save_fc_weight(w6, b6, \"fc6\", path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbb29341",
   "metadata": {},
   "source": [
    "# Saving Result without bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f95a94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1024)\n",
      "torch.Size([1, 32, 32])\n",
      "(32, 16, 16)\n",
      "(64, 8, 8)\n",
      "(128, 4, 4)\n",
      "(256, 2, 2)\n",
      "(1, 32)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "file = open('./result_new_wo_bias/conv1_input.txt','w')\n",
    "# x_original = torch.randn(1, 32,32)\n",
    "x_original = data_list[0].view(1,32,32)\n",
    "print((x_original == quant_signed_15(x_original)).sum())\n",
    "x_original = quant_signed_15(x_original)\n",
    "\n",
    "x = x_original\n",
    "x = x*16\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(x[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "b1 = np.zeros(32)\n",
    "x = x_original\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w1,b1, p=1)\n",
    "\n",
    "\n",
    "file = open('./result_new_wo_bias/conv1_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu1_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b2 = np.zeros(64)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w2,b2, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv2_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu2_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b3 = np.zeros(128)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w3,b3, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv3_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu3_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b4 = np.zeros(256)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w4,b4, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv4_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu4_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b5 = np.zeros(32)\n",
    "\n",
    "x = fc_fa_05(x,w5,b5, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/fc5_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(x)\n",
    "file = open('./result_new_wo_bias/relu5_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b6 = np.zeros(10)\n",
    "\n",
    "x = fc_fa_05(x,w6,b6, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/fc6_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(x)\n",
    "\n",
    "file = open('./result_new_wo_bias/relu6_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95ed748c",
   "metadata": {},
   "source": [
    "# Saving result with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd745e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1024)\n",
      "torch.Size([1, 32, 32])\n",
      "(32, 16, 16)\n",
      "(64, 8, 8)\n",
      "(128, 4, 4)\n",
      "(256, 2, 2)\n",
      "(1, 32)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w1, w2, w3, w4, w5, w6 = model.conv1.weight, model.conv2.weight, model.conv3.weight, model.conv4.weight, model.fc5.weight, model.fc6.weight\n",
    "b1, b2, b3, b4, b5, b6 = model.conv1.bias, model.conv2.bias, model.conv3.bias, model.conv4.bias, model.fc5.bias, model.fc6.bias\n",
    "\n",
    "w1 = w1.data.numpy()\n",
    "b1 = b1.data.numpy()\n",
    "\n",
    "w2 = w2.data.numpy()\n",
    "b2 = b2.data.numpy()\n",
    "\n",
    "w3 = w3.data.numpy()\n",
    "b3 = b3.data.numpy()\n",
    "\n",
    "w4 = w4.data.numpy()\n",
    "b4 = b4.data.numpy()\n",
    "\n",
    "w5 = w5.data.numpy()\n",
    "b5 = b5.data.numpy()\n",
    "\n",
    "w6 = w6.data.numpy()\n",
    "b6 = b6.data.numpy()\n",
    "\n",
    "\n",
    "\n",
    "file = open('./result_new_w_bias/conv1_input.txt','w')\n",
    "# x_original = torch.randn(1, 32,32)\n",
    "x_original = data_list[0].view(1,32,32)\n",
    "print((x_original == quant_signed_15(x_original)).sum())\n",
    "x_original = quant_signed_15(x_original)\n",
    "\n",
    "x = x_original\n",
    "x = x*8\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(x[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = x_original\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w1,b1, p=1)\n",
    "\n",
    "\n",
    "file = open('./result_new_w_bias/conv1_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu1_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w2,b2, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/conv2_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu2_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w3,b3, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/conv3_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu3_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w4,b4, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/conv4_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu4_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = fc_fa_05(x,w5,b5, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/fc5_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(x)\n",
    "file = open('./result_new_w_bias/relu5_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = fc_fa_05(x,w6,b6, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/fc6_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(x)\n",
    "\n",
    "file = open('./result_new_w_bias/relu6_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c3b6e5f",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f105fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83203125\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with Custom model\n",
    "correct = 0\n",
    "#randlist = random.sample(range(0, 10000), 256)\n",
    "randlist = [i for i in range(256)]\n",
    "for i in randlist:\n",
    "    \n",
    "    y = label_list[i]\n",
    "    x = data_list[i].view(1,32,32).numpy()\n",
    "\n",
    "    x = padding(x)\n",
    "    x = conv_custom_fa_05(x,w1,b1, p=1)\n",
    "    x = maxpooling(x)\n",
    "    x = relu(x)\n",
    "\n",
    "    x = padding(x)\n",
    "    x = conv_custom_fa_05(x,w2,b2, p=2)\n",
    "    x = maxpooling(x)\n",
    "    x = relu(x)\n",
    "\n",
    "    x = padding(x)\n",
    "    x = conv_custom_fa_05(x,w3,b3, p=3)\n",
    "    x = maxpooling(x)\n",
    "    x = relu(x)\n",
    "\n",
    "    x = padding(x)\n",
    "    x = conv_custom_fa_05(x,w4,b4, p=4)\n",
    "    x = maxpooling(x)\n",
    "    x = relu(x)\n",
    "\n",
    "    x = x.reshape(1,1024)\n",
    "\n",
    "    x = fc_fa_05(x,w5,b5, p=3)\n",
    "    x = relu(x)\n",
    "\n",
    "    x = fc_fa_05(x,w6,b6, p=1)\n",
    "\n",
    "\n",
    "    if (np.argmax(x) == y.item()):\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(randlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd919399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
