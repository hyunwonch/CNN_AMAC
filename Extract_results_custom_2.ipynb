{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfac8f03",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "673be2bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:07:42.404478Z",
     "start_time": "2023-04-19T05:07:42.377458Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Library\n",
    "import sys\n",
    "import os\n",
    "import os.path as pth\n",
    "\n",
    "#!pip install torchplot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import nets\n",
    "import datasets\n",
    "import tools\n",
    "import layers as L\n",
    "import train\n",
    "\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from slacker import Slacker\n",
    "from quantization import *\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200e405",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c9710fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:10:52.638887Z",
     "start_time": "2023-04-19T05:10:52.598168Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Custom functions - Completed\n",
    "\n",
    "@jit\n",
    "def amac(x, w):\n",
    "    return (x * w).sum()\n",
    "\n",
    "\n",
    "@jit\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "\n",
    "@jit\n",
    "def padding(a):\n",
    "    result = np.zeros((a.shape[0],a.shape[1]+2,a.shape[2]+2))\n",
    "    for i in range(a.shape[0]):\n",
    "        result[i] = np.pad(a[i],1)\n",
    "    return result\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def maxpooling(x_original):\n",
    "   \n",
    "    depth = x_original.shape[0] \n",
    "    row = int((x_original.shape[1])/2)\n",
    "    col = int((x_original.shape[2])/2)\n",
    "\n",
    "    one_layer = np.zeros((depth,row,col))\n",
    "    \n",
    "    for d in range(depth):\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                r = x_original[d,2*i:2*i+2,2*j:2*j+2].max()\n",
    "                one_layer[d,i,j] = r\n",
    "    return one_layer\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def quant_signed_05_np(original, bit=5):\n",
    "    bit = bit -1\n",
    "    original = np.clip(original, -0.9375, 0.9375)\n",
    "    original = original * (2**bit)\n",
    "    \n",
    "    (row, col) = original.shape\n",
    "    result = np.zeros((row,col))\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            result[i,j] = math.trunc(original[i,j])/ (2**bit)\n",
    "    return result\n",
    "\n",
    "@jit\n",
    "def quant_signed_05_np_fc(original, bit=5):\n",
    "    bit = bit -1\n",
    "    original = np.clip(original, -0.9375, 0.9375)\n",
    "    original = original * (2**bit)\n",
    "    \n",
    "    result = math.trunc(original)/ (2**bit)\n",
    "    return result\n",
    "\n",
    "@jit(nopython=True)\n",
    "def quant_signed_15_np(original, bit=5):\n",
    "    bit = bit -2\n",
    "    original = np.clip(original, -1.875, 1.875)\n",
    "    original = original * (2**bit)\n",
    "    \n",
    "    (row, col) = original.shape\n",
    "    result = np.zeros((row,col))\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            result[i,j] = math.trunc(original[i,j])/ (2**bit)\n",
    "    return result\n",
    "\n",
    "@jit\n",
    "def quant_signed_15_np_fc(original, bit=5):\n",
    "    bit = bit -2\n",
    "    original = np.clip(original,-1.875, 1.875)\n",
    "    original = original * (2**bit)\n",
    "    \n",
    "    result = math.trunc(original)/ (2**bit)\n",
    "    return result\n",
    "\n",
    "\n",
    "get_bin = lambda x, n: format(x, 'b').zfill(n).replace(\"-\",\"1\")\n",
    "\n",
    "# Partial sum function with point parameters - Completed\n",
    "\n",
    "# This is partial sum function for convolution layer -> Matches with verilog\n",
    "@jit(cache=True)\n",
    "def partial_sum_fa_conv_point(original, bit=5, point = 1):\n",
    "    a = original\n",
    "    bit = bit - 2  + (point)\n",
    "    value = 1.875/(2**(point))\n",
    "    \n",
    "    result = np.zeros(a.shape[0])\n",
    "    for d in range(a.shape[0]):\n",
    "        partial = a[d].sum()\n",
    "        if partial > value:\n",
    "            partial = value\n",
    "        elif partial < -value:\n",
    "            partial = -value\n",
    "        else:\n",
    "            partial = partial\n",
    "        result[d] = math.trunc(partial* (2**bit)) / (2**bit)\n",
    "    return result.sum()\n",
    "\n",
    "# This function matches with our verilog model & reasonable accuracy\n",
    "@jit(cache=True)\n",
    "def partial_sum_fa_fc_point(original, bit=5, point=1):\n",
    "    a = original\n",
    "    bit = bit - 2  + (point)\n",
    "    value = 1.875/(2**(point))\n",
    "    \n",
    "    partial = a.sum()\n",
    "    if partial > value:\n",
    "        partial = value\n",
    "    elif partial < -value:\n",
    "        partial = -value\n",
    "    else:\n",
    "        partial = partial\n",
    "    result= math.trunc(partial* (2**bit)) / (2**bit)\n",
    "    return result\n",
    "\n",
    "# FC & Conv function with point parameters - completed\n",
    "# These functions have same fixed point with bias & match with verilog file\n",
    "@jit(cache=True)\n",
    "def fc_fa_05(x_original, w, b, p=1):\n",
    "   \n",
    "    filt = w.shape[0]\n",
    "    stage = int(x_original.shape[1]/8)\n",
    "    c = np.zeros((1,filt))\n",
    "    for f in range(filt):\n",
    "        re = 0\n",
    "        for i in range(stage):\n",
    "            r = x_original[0,i*8:i*8+8] * w[f,i*8:i*8+8]\n",
    "            re = re + partial_sum_fa_fc_point(r, point=p)\n",
    "        c[0,f] = quant_signed_05_np_fc(re + b[f])\n",
    "    return c\n",
    "\n",
    "@jit(cache=True)\n",
    "def fc_fa(x_original, w, b, p=1):\n",
    "   \n",
    "    filt = w.shape[0]\n",
    "    stage = int(x_original.shape[1]/8)\n",
    "    c = np.zeros((1,filt))\n",
    "    for f in range(filt):\n",
    "        re = 0\n",
    "        for i in range(stage):\n",
    "            r = x_original[0,i*8:i*8+8] * w[f,i*8:i*8+8]\n",
    "            re = re + partial_sum_fa_fc_point(r, point=p)\n",
    "        c[0,f] = quant_signed_15_np_fc(re + b[f])\n",
    "    return c\n",
    "\n",
    "@jit(cache=True)\n",
    "def conv_custom_fa_05(x_original, w, b, p=1):\n",
    "   \n",
    "    filt = w.shape[0]\n",
    "    depth = x_original.shape[0] \n",
    "    row = x_original.shape[1] - 2\n",
    "    col = x_original.shape[2] - 2\n",
    "\n",
    "    c = np.zeros((filt,row,col))\n",
    "    one_layer = np.zeros((row,col))\n",
    "    \n",
    "    for f in range(filt):\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                r = x_original[:,i:i+3,j:j+3] * w[f]\n",
    "                one_layer[i,j] = partial_sum_fa_conv_point(r, point=p)\n",
    "        c[f,:,:] = quant_signed_05_np(one_layer + b[f])\n",
    "    return c\n",
    "\n",
    "\n",
    "@jit(cache=True)\n",
    "def conv_custom_fa(x_original, w, b, p=1):\n",
    "   \n",
    "    filt = w.shape[0]\n",
    "    depth = x_original.shape[0] \n",
    "    row = x_original.shape[1] - 2\n",
    "    col = x_original.shape[2] - 2\n",
    "\n",
    "    c = np.zeros((filt,row,col))\n",
    "    one_layer = np.zeros((row,col))\n",
    "    \n",
    "    for f in range(filt):\n",
    "        for i in range(row):\n",
    "            for j in range(col):\n",
    "                r = x_original[:,i:i+3,j:j+3] * w[f]\n",
    "                one_layer[i,j] = partial_sum_fa_conv_point(r, point=p)\n",
    "        c[f,:,:] = quant_signed_15_np(one_layer + b[f])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ed91364b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:26:25.564770Z",
     "start_time": "2023-04-19T05:26:25.528776Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function of saving parameter\n",
    "def save_conv_weight(weight, bias, file_name, path,include_bias = 1):\n",
    "    if(type(path) != str):\n",
    "        path = str(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory {path} created.\")\n",
    "    # else:\n",
    "    #     print(f\"Directory {path} already exists.\")\n",
    "    if(type(file_name) != str):\n",
    "        path = str(file_name)\n",
    "    if \".txt\" not in file_name:\n",
    "        weight_name = file_name + \"_weight.txt\"\n",
    "        bias_name = file_name + \"_bias.txt\"\n",
    "    file = open(path + \"/\" + weight_name,'w')\n",
    "\n",
    "    a = weight\n",
    "    a = a * 16\n",
    "    for d in range(a.shape[1]):\n",
    "        for f in range(a.shape[0]):\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    file.write(get_bin(int(a[f,d,i,j]),5)+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    file = open(path + \"/\" + bias_name,'w')\n",
    "    a = bias\n",
    "    a = a * 16\n",
    "    for i in range(a.shape[0]):\n",
    "        if include_bias:\n",
    "            file.write(get_bin(int(a[i]),5)+\"\\n\")\n",
    "        else:\n",
    "            file.write(\"0\\n\")\n",
    "    file.close()\n",
    "    print(f\"Save {file_name} weight to {path}/{weight_name}\\nSave {file_name} bias   to {path}/{bias_name}\")\n",
    "\n",
    "def save_fc_weight(weight, bias, file_name, path, include_bias = 1):\n",
    "    if(type(path) != str):\n",
    "        path = str(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory {path} created.\")\n",
    "    # else:\n",
    "    #     print(f\"Directory {path} already exists.\")\n",
    "    if(type(file_name) != str):\n",
    "        path = str(file_name)\n",
    "    if \".txt\" not in file_name:\n",
    "        file_name = file_name + \".txt\"\n",
    "    \n",
    "    file = open(path + \"/\" + file_name,'w')\n",
    "\n",
    "    weight = weight*16\n",
    "    bias = bias * 16\n",
    "    for w in range(0, weight.shape[0], 32):\n",
    "        for mac in range(min(weight.shape[0] - w, 32)):\n",
    "            if include_bias:\n",
    "                file.write(get_bin(int(bias[w+mac]),5)+\"\\n\")\n",
    "            else:\n",
    "                file.write(\"0\\n\")\n",
    "        for z in range(0, weight.shape[1], 8):\n",
    "            for mac in range(min(weight.shape[0] - w, 32)):\n",
    "                for port in range(8):\n",
    "                    file.write(get_bin(int(weight[w+mac,z+port]),5)+\"\\n\")\n",
    "                    \n",
    "    file.close()\n",
    "    print(f\"Save {file_name[:-4]} weight to {path}/{file_name}\")\n",
    "\n",
    "def save_conv_result(x, file_name, path):\n",
    "    if(type(path) != str):\n",
    "        path = str(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory {path} created.\")\n",
    "    # else:\n",
    "    #     print(f\"Directory {path} already exists.\")\n",
    "    if(type(file_name) != str):\n",
    "        path = str(file_name)\n",
    "    if \".txt\" not in file_name:\n",
    "        output_name = file_name + \"_output.txt\"\n",
    "    file = open(path + \"/\" + output_name,'w')\n",
    "\n",
    "    save = x*16\n",
    "    for d in range(x.shape[0]):\n",
    "        for i in range(x.shape[1]):\n",
    "            for j in range(x.shape[2]):\n",
    "                file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "    file.close()\n",
    "    \n",
    "def save_weight(model, nobias_file, bias_file):\n",
    "    for conv in [model.conv1, model.conv2, model.conv3, model.conv4]:\n",
    "        weight = conv.weight.detach().numpy()\n",
    "#         weight = quant_signed_05(weight)\n",
    "        print(weight.shape)\n",
    "        weight = weight * 16\n",
    "        bias = conv.bias.detach().numpy()\n",
    "#         bias = quant_signed_05(bias)\n",
    "        print(bias.shape)\n",
    "        bias = bias * 16\n",
    "        for w in range(0, weight.shape[0], 32):\n",
    "            for mac in range(32):\n",
    "                bias_file.write(get_bin(int(bias[w+mac]),5) +\"\\n\")\n",
    "                nobias_file.write(\"0\\n\")\n",
    "            for z in range(weight.shape[1]):\n",
    "                for mac in range(32):\n",
    "                    for i in range(3):\n",
    "                        for j in range(3):\n",
    "                            bias_file.write(get_bin(int(weight[w+mac,z,i,j]),5)+\"\\n\")\n",
    "                            nobias_file.write(get_bin(int(weight[w+mac,z,i,j]),5)+\"\\n\")\n",
    "                            \n",
    "    \n",
    "    for fc in [model.fc5, model.fc6]:\n",
    "        weight = fc.weight.detach().numpy()\n",
    "#         weight = quant_signed_05(weight)\n",
    "        print(weight.shape)\n",
    "        weight = weight * 16\n",
    "        bias = fc.bias.detach().numpy()\n",
    "#         bias = quant_signed_05(bias)\n",
    "        print(bias.shape)\n",
    "        bias = bias * 16\n",
    "        for w in range(0, weight.shape[0], 32):\n",
    "            for mac in range(min(weight.shape[0] - w, 32)):\n",
    "                bias_file.write(get_bin(int(bias[w+mac]),5) +\"\\n\")\n",
    "                nobias_file.write(\"0\\n\")\n",
    "            for z in range(0, weight.shape[1], 8):\n",
    "                for mac in range(min(weight.shape[0] - w, 32)):\n",
    "                    for port in range(8):\n",
    "                        bias_file.write(get_bin(int(weight[w+mac,z+port]),5)+\"\\n\")\n",
    "                        nobias_file.write(get_bin(int(weight[w+mac,z+port]),5)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96a068",
   "metadata": {},
   "source": [
    "# Open model and file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c7b1d12e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:11:29.712641Z",
     "start_time": "2023-04-19T05:11:29.604829Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quant MNIST\n",
      "########## Loaded checkpoint './checkpoints_quant/mnist_quant_mnist/2_18_Time_16_36/checkpoint_7_98.3.tar'\n",
      "mnist dataset with different fixed point\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and data\n",
    "\n",
    "file = open('latest.txt' , 'r' )\n",
    "line = file.readline()\n",
    "pretrained_checkpoint = line\n",
    "file.close()\n",
    "\n",
    "# Default settings for arch, dataset, and checkpoint\n",
    "arch = \"CNN_627_large\"\n",
    "dataset = \"cifar10\"\n",
    "batch_size = 256\n",
    "pretrained_checkpoint = pretrained_checkpoint\n",
    "\n",
    "# trainloader, _, testloader = datasets.get_mnist(batch_size)\n",
    "model = nets.mnist_quant()\n",
    "\n",
    "pretrained_ckpt = torch.load(pretrained_checkpoint)\n",
    "model.load_state_dict(pretrained_ckpt['state_dict'])\n",
    "print(\"########## Loaded checkpoint '{}'\".format(pretrained_checkpoint))\n",
    "\n",
    "print(\"mnist dataset with different fixed point\")\n",
    "with open(\"./data_quantized/quant_test_data_mnist.pkl\",\"rb\") as f:\n",
    "    data_list = pickle.load(f)\n",
    "with open(\"./data_quantized/quant_test_label_mnist.pkl\",\"rb\") as g:\n",
    "    label_list = pickle.load(g)\n",
    "    \n",
    "\n",
    "#print(\"mnist dataset with same fixed point\")\n",
    "#with open(\"./data_quantized/quant_test_data_mnist_05.pkl\",\"rb\") as f:\n",
    "#    data_list = pickle.load(f)\n",
    "#with open(\"./data_quantized/quant_test_label_mnist_05.pkl\",\"rb\") as g:\n",
    "#    label_list = pickle.load(g)\n",
    "    \n",
    "# Load Weights and biases\n",
    "\n",
    "w1, w2, w3, w4, w5, w6 = model.conv1.weight, model.conv2.weight, model.conv3.weight, model.conv4.weight, model.fc5.weight, model.fc6.weight\n",
    "b1, b2, b3, b4, b5, b6 = model.conv1.bias, model.conv2.bias, model.conv3.bias, model.conv4.bias, model.fc5.bias, model.fc6.bias\n",
    "\n",
    "#w1 = quant_signed_15(w1)\n",
    "#w2 = quant_signed_15(w2)\n",
    "#w3 = quant_signed_15(w3)\n",
    "#w4 = quant_signed_15(w4)\n",
    "#w5 = quant_signed_15(w5)\n",
    "#w6 = quant_signed_15(w6)\n",
    "\n",
    "\n",
    "w1 = w1.data.numpy()\n",
    "b1 = b1.data.numpy()\n",
    "\n",
    "w2 = w2.data.numpy()\n",
    "b2 = b2.data.numpy()\n",
    "\n",
    "w3 = w3.data.numpy()\n",
    "b3 = b3.data.numpy()\n",
    "\n",
    "w4 = w4.data.numpy()\n",
    "b4 = b4.data.numpy()\n",
    "\n",
    "w5 = w5.data.numpy()\n",
    "b5 = b5.data.numpy()\n",
    "\n",
    "w6 = w6.data.numpy()\n",
    "b6 = b6.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9b0545a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:18:51.210123Z",
     "start_time": "2023-04-19T05:18:50.269173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1, 3, 3)\n",
      "(32,)\n",
      "(64, 32, 3, 3)\n",
      "(64,)\n",
      "(128, 64, 3, 3)\n",
      "(128,)\n",
      "(256, 128, 3, 3)\n",
      "(256,)\n",
      "(32, 1024)\n",
      "(32,)\n",
      "(10, 32)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "file_no = open('./hope/weight_no_bias.txt','w')\n",
    "file_bias = open('./hope/weight_bias.txt','w')\n",
    "save_weight(model, file_no,file_bias)\n",
    "\n",
    "file_no.close()\n",
    "file_bias.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec55b8a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T04:20:48.208927Z",
     "start_time": "2023-04-19T04:20:47.731511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save conv1 weight to weight_new_wo_bias/conv1_weight.txt\n",
      "Save conv1 bias   to weight_new_wo_bias/conv1_bias.txt\n",
      "Save conv2 weight to weight_new_wo_bias/conv2_weight.txt\n",
      "Save conv2 bias   to weight_new_wo_bias/conv2_bias.txt\n",
      "Save conv3 weight to weight_new_wo_bias/conv3_weight.txt\n",
      "Save conv3 bias   to weight_new_wo_bias/conv3_bias.txt\n",
      "Save conv4 weight to weight_new_wo_bias/conv4_weight.txt\n",
      "Save conv4 bias   to weight_new_wo_bias/conv4_bias.txt\n",
      "Save fc5 weight to weight_new_wo_bias/fc5.txt\n",
      "Save fc6 weight to weight_new_wo_bias/fc6.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"weight_new_wo_bias\"\n",
    "\n",
    "save_conv_weight(w1, b1, \"conv1\", path,include_bias=0)\n",
    "save_conv_weight(w2, b2, \"conv2\", path,include_bias=0)\n",
    "save_conv_weight(w3, b3, \"conv3\", path,include_bias=0)\n",
    "save_conv_weight(w4, b4, \"conv4\", path,include_bias=0)\n",
    "save_fc_weight(w5, b5, \"fc5\", path,include_bias=0)\n",
    "save_fc_weight(w6, b6, \"fc6\", path,include_bias=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0f98327e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T04:20:49.235593Z",
     "start_time": "2023-04-19T04:20:48.782285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save conv1 weight to weight_new_w_bias/conv1_weight.txt\n",
      "Save conv1 bias   to weight_new_w_bias/conv1_bias.txt\n",
      "Save conv2 weight to weight_new_w_bias/conv2_weight.txt\n",
      "Save conv2 bias   to weight_new_w_bias/conv2_bias.txt\n",
      "Save conv3 weight to weight_new_w_bias/conv3_weight.txt\n",
      "Save conv3 bias   to weight_new_w_bias/conv3_bias.txt\n",
      "Save conv4 weight to weight_new_w_bias/conv4_weight.txt\n",
      "Save conv4 bias   to weight_new_w_bias/conv4_bias.txt\n",
      "Save fc5 weight to weight_new_w_bias/fc5.txt\n",
      "Save fc6 weight to weight_new_w_bias/fc6.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"weight_new_w_bias\"\n",
    "\n",
    "save_conv_weight(w1, b1, \"conv1\", path)\n",
    "save_conv_weight(w2, b2, \"conv2\", path)\n",
    "save_conv_weight(w3, b3, \"conv3\", path)\n",
    "save_conv_weight(w4, b4, \"conv4\", path)\n",
    "save_fc_weight(w5, b5, \"fc5\", path)\n",
    "save_fc_weight(w6, b6, \"fc6\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a47c6c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T04:19:48.338675Z",
     "start_time": "2023-04-19T04:19:48.321224Z"
    }
   },
   "outputs": [],
   "source": [
    "w1, w2, w3, w4, w5, w6 = model.conv1.weight, model.conv2.weight, model.conv3.weight, model.conv4.weight, model.fc5.weight, model.fc6.weight\n",
    "b1, b2, b3, b4, b5, b6 = model.conv1.bias, model.conv2.bias, model.conv3.bias, model.conv4.bias, model.fc5.bias, model.fc6.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17795b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T01:49:00.392512Z",
     "start_time": "2023-04-18T01:49:00.381512Z"
    }
   },
   "source": [
    "# original\n",
    "- bias   : _ . _ _ _ _\n",
    "\n",
    "- input  : _ _ . _ _ _\n",
    "- wegith : _ . _ _ _ _\n",
    "- output : _ | _ _ . _ _ _  | _ _ _ _\n",
    "- output : _ _ . _ _ _\n",
    " \n",
    "- conv   : _ _ . _ _ _\n",
    "- wegith : _ . _ _ _ _\n",
    "\n",
    "\n",
    "# solution (problem) -> Solved\n",
    "- bias   : _ . _ _ _ _\n",
    "- wegith : _ . _ _ _ _\n",
    "- input  : _ _ . _ _ _\n",
    "- output : _ _ | _ . _ _ _ _ | _ _ _\n",
    "\n",
    "- conv   : _ . _ _ _ _\n",
    "- wegith : _ . _ _ _ _\n",
    "- output : _ _ | _ . _ _ _ _ | _ _ _\n",
    "- output : _ | _ _ . _ _ _ |_ _ _ _\n",
    "-> Play with Accumulation layer\n",
    "\n",
    "# solution2 \n",
    "- bias   : _ . _ _ _ _\n",
    "- wegith : _ _ . _ _ _\n",
    "- input  : _ . _ _ _ _\n",
    "- output : _ _ | _ . _ _ _ _ | _ _ _\n",
    "\n",
    "- conv   : _ . _ _ _ _\n",
    "\n",
    "# solution3 \n",
    "- bias   : _ . _ _ _ _\n",
    "- wegith : _ . _ _ _ _\n",
    "- input  : _ . _ _ _ _\n",
    "- output : _ | _ . _ _ _ _ | _ _ _ _\n",
    "- conv   : _ . _ _ _ _\n",
    "\n",
    "\n",
    "\n",
    "# potential solution\n",
    "- bias   : _ . _ _ _ _\n",
    "- wegith : _ . _ _ _ _\n",
    "- input  : _ . _ _ _ _\n",
    "- output : _ | _ . _ _ _ _ | _ _ _ _\n",
    "\n",
    "- conv   : _ . _ _ _ _\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91fb179",
   "metadata": {},
   "source": [
    "# Comparing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fdb3233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T03:25:13.485526Z",
     "start_time": "2023-04-19T03:25:13.461192Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('./result_new_wo_bias/conv1_input.txt','r')\n",
    "file2 = open('./data_extracted/conv1_input_0317.txt','r')\n",
    "for i,j in zip(file,file2):\n",
    "    if i.replace('\\n','') != j.replace('\\n',''):\n",
    "        print(i.replace('\\n',''), j.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e95abfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:19:25.813602Z",
     "start_time": "2023-04-19T05:19:25.776991Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('./result_new_wo_bias/conv1_output.txt','r')\n",
    "file2 = open('./data_extracted/conv1_output_0317.txt','r')\n",
    "for i,j in zip(file,file2):\n",
    "    if i.replace('\\n','') != j.replace('\\n',''):\n",
    "        pass#print(i.replace('\\n',''), j.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e206d1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T03:25:25.803032Z",
     "start_time": "2023-04-19T03:25:25.803032Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('./result_new_wo_bias/pool_relu1_output.txt','r')\n",
    "file2 = open('./data_extracted/pool_relu1_output_0317.txt','r')\n",
    "for i,j in zip(file,file2):\n",
    "    print(i.replace('\\n',''), j.replace('\\n',''))\n",
    "    #if i.replace('\\n','') != j.replace('\\n',''):\n",
    "        #print(i.replace('\\n',''), j.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "35294e18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:19:34.525635Z",
     "start_time": "2023-04-19T05:19:34.501671Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('./result_new_wo_bias/conv2_output.txt','r')\n",
    "file2 = open('./data_extracted/conv2_output_0317.txt','r')\n",
    "for i,j in zip(file,file2):\n",
    "    # print(i.replace('\\n',''), j.replace('\\n',''))\n",
    "    if i.replace('\\n','') != j.replace('\\n',''):\n",
    "        pass#print(i.replace('\\n',''), j.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b09f4fbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:19:37.109656Z",
     "start_time": "2023-04-19T05:19:37.088566Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('./result_new_wo_bias/conv3_output.txt','r')\n",
    "file2 = open('./data_extracted/conv3_output_0317.txt','r')\n",
    "num = 1\n",
    "for i,j in zip(file,file2):\n",
    "    #print(num, i.replace('\\n',''), j.replace('\\n',''))\n",
    "    num += 1\n",
    "    #if i.replace('\\n','') != j.replace('\\n',''):\n",
    "    #    print(i.replace('\\n',''), j.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cc3b86ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:19:43.096445Z",
     "start_time": "2023-04-19T05:19:43.082419Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open('./result_new_wo_bias/pool_relu1_output.txt','r')\n",
    "file2 = open('./data_extracted/pool_relu1_output_0317.txt','r')\n",
    "for i,j in zip(file,file2):\n",
    "    pass#print(i.replace('\\n',''), j.replace('\\n',''))\n",
    "    #if i.replace('\\n','') != j.replace('\\n',''):\n",
    "        #print(i.replace('\\n',''), j.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3187a5cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:11:49.420742Z",
     "start_time": "2023-04-19T05:11:49.412667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1024)\n"
     ]
    }
   ],
   "source": [
    "x_original = data_list[0].view(1,32,32)\n",
    "print((x_original == quant_signed_15(x_original)).sum())\n",
    "x_original = quant_signed_15(x_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df5aca",
   "metadata": {},
   "source": [
    "# Savine Result w/o bias hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c49345fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:50:10.586938Z",
     "start_time": "2023-04-19T05:50:10.324509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1024)\n",
      "torch.Size([1, 32, 32])\n",
      "(32, 16, 16)\n",
      "(64, 8, 8)\n",
      "(128, 4, 4)\n",
      "(256, 2, 2)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(1, 32)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# x_original = torch.randn(1, 32,32)\n",
    "x_original = data_list[0].view(1,32,32)\n",
    "print((x_original == quant_signed_15(x_original)).sum())\n",
    "x_original = quant_signed_15(x_original)\n",
    "\n",
    "x = x_original\n",
    "x = x*8\n",
    "print(x.shape)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv1_input.txt','w')\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(x[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "b1 = np.zeros(32)\n",
    "x = x_original\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w1,b1, p=1)\n",
    "\n",
    "\n",
    "file = open('./result_new_wo_bias/conv1_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu1_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b2 = np.zeros(64)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w2,b2, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv2_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu2_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b3 = np.zeros(128)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w3,b3, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv3_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu3_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b4 = np.zeros(256)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w4,b4, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv4_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu4_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b5 = np.zeros(32)\n",
    "\n",
    "x = fc_fa(x,w5,b5, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/fc5_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(x)\n",
    "file = open('./result_new_wo_bias/relu5_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b6 = np.zeros(10)\n",
    "\n",
    "x = fc_fa(x,w6,b6, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/fc6_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(x)\n",
    "\n",
    "file = open('./result_new_wo_bias/relu6_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e187bf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:16:41.996619Z",
     "start_time": "2023-04-19T05:16:41.984237Z"
    }
   },
   "source": [
    "# Savie result with bias hope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0a3978d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:39:03.144548Z",
     "start_time": "2023-04-19T05:39:02.931425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1024)\n",
      "torch.Size([1, 32, 32])\n",
      "(32, 16, 16)\n",
      "(64, 8, 8)\n",
      "(128, 4, 4)\n",
      "(256, 2, 2)\n",
      "(1, 32)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "w1, w2, w3, w4, w5, w6 = model.conv1.weight, model.conv2.weight, model.conv3.weight, model.conv4.weight, model.fc5.weight, model.fc6.weight\n",
    "b1, b2, b3, b4, b5, b6 = model.conv1.bias, model.conv2.bias, model.conv3.bias, model.conv4.bias, model.fc5.bias, model.fc6.bias\n",
    "\n",
    "\n",
    "w1 = w1.data.numpy()\n",
    "b1 = b1.data.numpy()\n",
    "\n",
    "w2 = w2.data.numpy()\n",
    "b2 = b2.data.numpy()\n",
    "\n",
    "w3 = w3.data.numpy()\n",
    "b3 = b3.data.numpy()\n",
    "\n",
    "w4 = w4.data.numpy()\n",
    "b4 = b4.data.numpy()\n",
    "\n",
    "w5 = w5.data.numpy()\n",
    "b5 = b5.data.numpy()\n",
    "\n",
    "w6 = w6.data.numpy()\n",
    "b6 = b6.data.numpy()\n",
    "\n",
    "\n",
    "\n",
    "file = open('./result_new_w_bias/conv1_input.txt','w')\n",
    "# x_original = torch.randn(1, 32,32)\n",
    "x_original = data_list[0].view(1,32,32)\n",
    "print((x_original == quant_signed_15(x_original)).sum())\n",
    "x_original = quant_signed_15(x_original)\n",
    "\n",
    "x = x_original\n",
    "x = x*8\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(x[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = x_original\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w1,b1, p=1)\n",
    "\n",
    "\n",
    "file = open('./result_new_w_bias/conv1_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu1_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w2,b2, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/conv2_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu2_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w3,b3, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/conv3_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu3_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w4,b4, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/conv4_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu4_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = fc_fa(x,w5,b5, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/fc5_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(x)\n",
    "file = open('./result_new_w_bias/relu5_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = fc_fa(x,w6,b6, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/fc6_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(x)\n",
    "\n",
    "file = open('./result_new_w_bias/relu6_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb29341",
   "metadata": {},
   "source": [
    "# Saving Result without bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7f95a94e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T04:22:43.637580Z",
     "start_time": "2023-04-19T04:22:43.386192Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1024)\n",
      "torch.Size([1, 32, 32])\n",
      "(32, 16, 16)\n",
      "(64, 8, 8)\n",
      "(128, 4, 4)\n",
      "(256, 2, 2)\n",
      "(1, 32)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "file = open('./result_new_wo_bias/conv1_input.txt','w')\n",
    "# x_original = torch.randn(1, 32,32)\n",
    "x_original = data_list[0].view(1,32,32)\n",
    "print((x_original == quant_signed_15(x_original)).sum())\n",
    "x_original = quant_signed_15(x_original)\n",
    "\n",
    "x = x_original\n",
    "x = x*8\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(x[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "b1 = np.zeros(32)\n",
    "x = x_original\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w1,b1, p=1)\n",
    "\n",
    "\n",
    "file = open('./result_new_wo_bias/conv1_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu1_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b2 = np.zeros(64)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w2,b2, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv2_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu2_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b3 = np.zeros(128)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w3,b3, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv3_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu3_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b4 = np.zeros(256)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w4,b4, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv4_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_wo_bias/pool_relu4_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b5 = np.zeros(32)\n",
    "\n",
    "x = fc_fa_05(x,w5,b5, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/fc5_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(x)\n",
    "file = open('./result_new_wo_bias/relu5_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "b6 = np.zeros(10)\n",
    "\n",
    "x = fc_fa_05(x,w6,b6, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/fc6_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(x)\n",
    "\n",
    "file = open('./result_new_wo_bias/relu6_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed748c",
   "metadata": {},
   "source": [
    "# Saving result with bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bd745e75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T04:22:47.656291Z",
     "start_time": "2023-04-19T04:22:47.369034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1024)\n",
      "torch.Size([1, 32, 32])\n",
      "(32, 16, 16)\n",
      "(64, 8, 8)\n",
      "(128, 4, 4)\n",
      "(256, 2, 2)\n",
      "(1, 32)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "w1, w2, w3, w4, w5, w6 = model.conv1.weight, model.conv2.weight, model.conv3.weight, model.conv4.weight, model.fc5.weight, model.fc6.weight\n",
    "b1, b2, b3, b4, b5, b6 = model.conv1.bias, model.conv2.bias, model.conv3.bias, model.conv4.bias, model.fc5.bias, model.fc6.bias\n",
    "\n",
    "w1 = quant_signed_15(w1)\n",
    "w2 = quant_signed_15(w2)\n",
    "w3 = quant_signed_15(w3)\n",
    "w4 = quant_signed_15(w4)\n",
    "w5 = quant_signed_15(w5)\n",
    "w6 = quant_signed_15(w6)\n",
    "\n",
    "w1 = w1.data.numpy()\n",
    "b1 = b1.data.numpy()\n",
    "\n",
    "w2 = w2.data.numpy()\n",
    "b2 = b2.data.numpy()\n",
    "\n",
    "w3 = w3.data.numpy()\n",
    "b3 = b3.data.numpy()\n",
    "\n",
    "w4 = w4.data.numpy()\n",
    "b4 = b4.data.numpy()\n",
    "\n",
    "w5 = w5.data.numpy()\n",
    "b5 = b5.data.numpy()\n",
    "\n",
    "w6 = w6.data.numpy()\n",
    "b6 = b6.data.numpy()\n",
    "\n",
    "\n",
    "\n",
    "file = open('./result_new_w_bias/conv1_input.txt','w')\n",
    "# x_original = torch.randn(1, 32,32)\n",
    "x_original = data_list[0].view(1,32,32)\n",
    "print((x_original == quant_signed_05(x_original)).sum())\n",
    "x_original = quant_signed_05(x_original)\n",
    "\n",
    "x = x_original\n",
    "x = x*16\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(x[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = x_original\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w1,b1, p=1)\n",
    "\n",
    "\n",
    "file = open('./result_new_w_bias/conv1_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu1_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w2,b2, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/conv2_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu2_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w3,b3, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/conv3_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu3_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "x = padding(x)\n",
    "x = conv_custom_fa_05(x,w4,b4, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/conv4_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(maxpooling(x))\n",
    "\n",
    "file = open('./result_new_w_bias/pool_relu4_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            #print(x[d,i,j])\n",
    "            file.write(get_bin(int(save[d,i,j]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = fc_fa_05(x,w5,b5, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/fc5_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "x = relu(x)\n",
    "file = open('./result_new_w_bias/relu5_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = fc_fa_05(x,w6,b6, p=1)\n",
    "\n",
    "file = open('./result_new_w_bias/fc6_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(x)\n",
    "\n",
    "file = open('./result_new_w_bias/relu6_output.txt','w')\n",
    "\n",
    "save = x*16\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b6e5f",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5f105fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:30:18.277673Z",
     "start_time": "2023-04-19T05:29:56.338104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8203125\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with Custom model\n",
    "correct = 0\n",
    "#randlist = [i for i in range(50)]\n",
    "#randlist = random.sample(range(0, 10000), 10)\n",
    "randlist = [i for i in range(256)]\n",
    "for i in randlist:\n",
    "    y = label_list[i]\n",
    "    x = data_list[i].view(1,32,32).numpy()\n",
    "\n",
    "    x = padding(x)\n",
    "    x = conv_custom_fa(x,w1,b1, p=1)\n",
    "    x = maxpooling(x)\n",
    "    x = relu(x)\n",
    "\n",
    "    x = padding(x)\n",
    "    x = conv_custom_fa(x,w2,b2, p=3)#3\n",
    "    x = maxpooling(x)\n",
    "    x = relu(x)\n",
    "\n",
    "    x = padding(x)\n",
    "    x = conv_custom_fa(x,w3,b3, p=4)#4\n",
    "    x = maxpooling(x)\n",
    "    x = relu(x)\n",
    "\n",
    "    x = padding(x)\n",
    "    x = conv_custom_fa(x,w4,b4, p=4)#4\n",
    "    x = maxpooling(x)\n",
    "    x = relu(x)\n",
    "\n",
    "    x = x.reshape(1,1024)\n",
    "\n",
    "    x = fc_fa(x,w5,b5, p=4)#4\n",
    "    x = relu(x)\n",
    "\n",
    "    x = fc_fa(x,w6,b6, p=1)#1\n",
    "\n",
    "\n",
    "    if (np.argmax(x) == y.item()):\n",
    "        correct += 1\n",
    "\n",
    "print(correct/len(randlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0cab3398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:45:34.580705Z",
     "start_time": "2023-04-19T05:45:34.562387Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "10873b33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:48:56.167770Z",
     "start_time": "2023-04-19T05:48:56.163769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x_original = data_list[0].view(1,32,32)\n",
    "x = data_list[i].view(1,32,32).numpy()\n",
    "print(x_original == x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba2710d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a53b5558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T05:56:20.916533Z",
     "start_time": "2023-04-19T05:56:20.695651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n",
      "[[0.25 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.5  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "b1 = np.zeros(32)\n",
    "b2 = np.zeros(64)\n",
    "b3 = np.zeros(128)\n",
    "b4 = np.zeros(256)\n",
    "b5 = np.zeros(32)\n",
    "b6 = np.zeros(10)\n",
    "\n",
    "i=0\n",
    "y = label_list[i]\n",
    "x_original = data_list[0].view(1,32,32)\n",
    "x = data_list[i].view(1,32,32)\n",
    "x1 = quant_signed_15(x_original)\n",
    "print(x==x1)\n",
    "\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w1,b1, p=1)\n",
    "\n",
    "file = open('./result_new_wo_bias/conv1_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = maxpooling(x)\n",
    "x = relu(x)\n",
    "\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w2,b2, p=1)#3\n",
    "\n",
    "file = open('./result_new_wo_bias/conv2_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "x = maxpooling(x)\n",
    "x = relu(x)\n",
    "\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w3,b3, p=1)#4\n",
    "\n",
    "file = open('./result_new_wo_bias/conv3_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "x = maxpooling(x)\n",
    "x = relu(x)\n",
    "\n",
    "x = padding(x)\n",
    "x = conv_custom_fa(x,w4,b4, p=1)#4\n",
    "\n",
    "file = open('./result_new_wo_bias/conv4_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "        for j in range(x.shape[2]):\n",
    "            file.write(get_bin(int(save[d,i,j]),10)+\"\\n\")\n",
    "file.close()\n",
    "x = maxpooling(x)\n",
    "x = relu(x)\n",
    "x = x.reshape(1,1024)\n",
    "\n",
    "x = fc_fa(x,w5,b5, p=1)#4\n",
    "file = open('./result_new_wo_bias/fc5_output.txt','w')\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(x)\n",
    "\n",
    "file = open('./result_new_wo_bias/relu5_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()\n",
    "print(x)\n",
    "x = fc_fa(x,w6,b6, p=1)#1\n",
    "file = open('./result_new_wo_bias/fc6_output.txt','w')\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),10)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "x = relu(x)\n",
    "file = open('./result_new_wo_bias/relu6_output.txt','w')\n",
    "\n",
    "save = x*8\n",
    "for d in range(x.shape[0]):\n",
    "    for i in range(x.shape[1]):\n",
    "            file.write(get_bin(int(save[d,i]),5)+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf869c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "696.8px",
    "left": "76px",
    "top": "110.1px",
    "width": "307.192px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
